{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Percona Operator for PostgreSQL documentation","text":"<p>The Percona Operator for PostgreSQL  automates the creation, modification, or deletion of items in your Percona Distribution for PostgreSQL environment. The Operator contains the necessary Kubernetes settings to maintain a consistent PostgreSQL cluster.</p> <p>Percona Kubernetes Operator is based on best practices for configuration and setup of a Percona Distribution for PostgreSQL cluster. The benefits of the Operator are many, but saving time and delivering a consistent and vetted environment is key.</p> <p>Starting with Percona Kubernetes Operator is easy. Follow our documentation guides, and you\u2019ll be set up in a minute.</p>"},{"location":"index.html#installation-guides","title":"Installation guides","text":"<p>Want to see it for yourself? Get started quickly with our step-by-step installation instructions.</p> <p>Quickstart guides </p>"},{"location":"index.html#security-and-encryption","title":"Security and encryption","text":"<p>Rest assured! Learn more about our security features designed to protect your valuable data.</p> <p>Security measures </p>"},{"location":"index.html#backup-management","title":"Backup management","text":"<p>Learn what you can do to maintain regular backups of your PostgrgeSQL cluster.</p> <p>Backup management </p>"},{"location":"index.html#troubleshooting","title":"Troubleshooting","text":"<p>Our comprehensive resources will help you overcome challenges, from everyday issues to specific doubts.</p> <p>Diagnostics </p>"},{"location":"index.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"System-Requirements.html","title":"System Requirements","text":"<p>The Operator is validated for deployment on Kubernetes, GKE and EKS clusters. The Operator is cloud native and storage agnostic, working with a wide variety of storage classes, hostPath, and NFS.</p>"},{"location":"System-Requirements.html#officially-supported-platforms","title":"Officially supported platforms","text":"<p>The Operator was developed and tested with PostgreSQL versions 12.16, 13.12, and 14.9. Other options may also work but have not been tested. The Operator {{ release }} provides connection pooling based on pgBouncer 1.20.0 and high-availability implementation based on Patroni 2.1.4.</p> <p>The following platforms were tested and are officially supported by the Operator {{ release }}:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.24 - 1.28</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.24 - 1.28</p> </li> <li> <p>OpenShift 4.11 - 4.14</p> </li> <li> <p>Minikube 1.32</p> </li> </ul> <p>Other Kubernetes platforms may also work but have not been tested.</p>"},{"location":"System-Requirements.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"TLS.html","title":"Transport Layer Security (TLS)","text":"<p>The Percona Operator for PostgreSQL uses Transport Layer Security (TLS) cryptographic protocol for the following types of communication:</p> <ul> <li>Internal - communication between PostgreSQL instances in the cluster</li> <li>External - communication between the client application and the cluster</li> </ul> <p>The internal certificate is also used as an authorization method for PostgreSQL Replica instances.</p> <p>TLS security can be configured in several ways:</p> <ul> <li>the Operator can generate certificates automatically at cluster creation time,</li> <li>you can also generate certificates manually.</li> </ul> <p>You can also use pre-generated certificates available in the <code>deploy/ssl-secrets.yaml</code> file for test purposes, but we strongly recommend avoiding their usage on any production system!</p> <p>The following subsections explain how to configure TLS security with the Operator yourself, as well as how to temporarily disable it if needed.</p>"},{"location":"TLS.html#allow-the-operator-to-generate-certificates-automatically","title":"Allow the Operator to generate certificates automatically","text":"<p>The Operator is able to generate long-term certificates automatically and turn on encryption at cluster creation time, if there are no certificate secrets available. It generates certificates with the help of cert-manager -  a Kubernetes certificate management controller widely used to automate the management and issuance of TLS certificates. Cert-manager is community-driven and open source.</p>"},{"location":"TLS.html#installation-of-the-cert-manager","title":"Installation of the cert-manager","text":"<p>You can install cert-manager as follows:</p> <ul> <li>Create a namespace,</li> <li>Disable resource validations on the cert-manager namespace,</li> <li>Install the cert-manager.</li> </ul> <p>The following commands perform all the needed actions:</p> <pre><code>$ kubectl create namespace cert-manager\n$ kubectl label namespace cert-manager certmanager.k8s.io/disable-validation=true\n$ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v{{ certmanagerversion }}/cert-manager.yaml\n</code></pre> <p>After the installation, you can verify the cert-manager by running the following command:</p> <pre><code>$ kubectl get pods -n cert-manager\n</code></pre> <p>The result should display the cert-manager and webhook active and running.</p>"},{"location":"TLS.html#turning-automatic-generation-of-certificates-on","title":"Turning automatic generation of certificates on","text":"<p>When you have already installed cert-manager, the operator is able to request a certificate from it. To make this happend, uncomment <code>sslCA</code>, <code>sslSecretName</code>, and <code>sslReplicationSecretName</code> options in the <code>deploy/cr.yaml</code> configuration file:</p> <pre><code>...\nspec:\n#  secretsName: cluster1-users\n  sslCA: cluster1-ssl-ca\n  sslSecretName: cluster1-ssl-keypair\n  sslReplicationSecretName: cluster1-ssl-keypair\n...\n</code></pre> <p>When done, deploy your cluster as usual, with the <code>kubectl apply -f deploy/cr.yaml</code> command. Certificates will be generated if there are no certificate secrets available.</p>"},{"location":"TLS.html#generate-certificates-manually","title":"Generate certificates manually","text":"<p>To generate certificates manually, follow these steps:</p> <ol> <li>Provision a  to generate TLS certificates,</li> <li>Generate a  key and certificate file with the server details,</li> <li>Create the server TLS certificates using the keys, certs, and server details.</li> </ol> <p>The set of commands generates certificates with the following attributes:</p> <ul> <li><code>Server-pem</code> - Certificate</li> <li><code>Server-key.pem</code> - the private key</li> <li><code>ca.pem</code> - Certificate Authority</li> </ul> <p>You should generate one set of certificates for external communications, and another set for internal ones.</p> <p>Supposing that your cluster name is <code>cluster1</code>, you can use the following commands to generate certificates:</p> <pre><code>$ CLUSTER_NAME=cluster1\n$ NAMESPACE=default\n$ cat &lt;&lt;EOF | cfssl gencert -initca - | cfssljson -bare ca\n{\n  \"CN\": \"*\",\n  \"key\": {\n    \"algo\": \"ecdsa\",\n    \"size\": 384\n  }\n}\nEOF\n\n$ cat &lt;&lt;EOF &gt; ca-config.json\n{\n   \"signing\": {\n     \"default\": {\n        \"expiry\": \"87600h\",\n        \"usages\": [\"digital signature\", \"key encipherment\", \"content commitment\"]\n      }\n   }\n}\nEOF\n\n$ cat &lt;&lt;EOF | cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=./ca-config.json - | cfssljson -bare server\n{\n   \"hosts\": [\n     \"localhost\",\n     \"${CLUSTER_NAME}\",\n     \"${CLUSTER_NAME}.${NAMESPACE}\",\n     \"${CLUSTER_NAME}.${NAMESPACE}.svc.cluster.local\",\n     \"${CLUSTER_NAME}-pgbouncer\",\n     \"${CLUSTER_NAME}-pgbouncer.${NAMESPACE}\",\n     \"${CLUSTER_NAME}-pgbouncer.${NAMESPACE}.svc.cluster.local\",\n     \"*.${CLUSTER_NAME}\",\n     \"*.${CLUSTER_NAME}.${NAMESPACE}\",\n     \"*.${CLUSTER_NAME}.${NAMESPACE}.svc.cluster.local\",\n     \"*.${CLUSTER_NAME}-pgbouncer\",\n     \"*.${CLUSTER_NAME}-pgbouncer.${NAMESPACE}\",\n     \"*.${CLUSTER_NAME}-pgbouncer.${NAMESPACE}.svc.cluster.local\"\n   ],\n   \"CN\": \"${CLUSTER_NAME}\",\n   \"key\": {\n     \"algo\": \"ecdsa\",\n     \"size\": 384\n   }\n}\nEOF\n\n$ kubectl create secret generic ${CLUSTER_NAME}-ssl-ca --from-file=ca.crt=ca.pem\n$ kubectl create secret tls  ${CLUSTER_NAME}-ssl-keypair --cert=server.pem --key=server-key.pem\n</code></pre> <p>If your PostgreSQL cluster includes replica instances (this feature is on by default), generate certificates for them in a similar way:</p> <pre><code>$ cat &lt;&lt;EOF | cfssl gencert -ca=ca.pem  -ca-key=ca-key.pem -config=./ca-config.json - | cfssljson -bare replicas\n{\n   \"CN\": \"primaryuser\",\n   \"key\": {\n      \"algo\": \"ecdsa\",\n      \"size\": 384\n   }\n}\nEOF\n\n$ kubectl create secret tls  ${CLUSTER_NAME}-ssl-replicas --cert=replicas.pem --key=replicas-key.pem\n</code></pre> <p>When certificates are generated, set the following keys in the <code>deploy/cr.yaml</code> configuration file:</p> <ul> <li><code>spec.sslCA</code> key should contain the name of the secret with TLS  used for both connection encryption (external traffic), and replication (internal traffic),</li> <li><code>spec.sslSecretName</code> key should contain the name of the secret created to encrypt external communications,</li> <li><code>spec.secrets.sslReplicationSecretName</code> key should contain the name of the secret created to encrypt internal communications,</li> <li><code>spec.tlsOnly</code> key set to <code>true</code> enforces encryption</li> </ul> <p>Don\u2019t forget to apply changes as usual:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre>"},{"location":"TLS.html#check-connectivity-to-the-cluster","title":"Check connectivity to the cluster","text":"<p>You can check TLS communication with use of the <code>psql</code>, the standard interactive terminal-based frontend to PostgreSQL. The following command will spawn a new <code>pg-client</code> container, which includes needed command and can be used for the check (use your real cluster name instead of the <code>&lt;cluster-name&gt;</code> placeholder):</p> <pre><code>$ cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pg-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: pg-client\n  template:\n    metadata:\n      labels:\n        name: pg-client\n    spec:\n      containers:\n        - name: pg-client\n          image: perconalab/percona-distribution-postgresql:{{ postgresrecommended }}\n          imagePullPolicy: Always\n          command:\n          - sleep\n          args:\n          - \"100500\"\n          volumeMounts:\n            - name: ca\n              mountPath: \"/tmp/tls\"\n      volumes:\n      - name: ca\n        secret:\n          secretName: &lt;cluster_name&gt;-ssl-ca\n          items:\n          - key: ca.crt\n            path: ca.crt\n            mode: 0777\nEOF\n</code></pre> <p>Now get shell access to the newly created container, and launch the PostgreSQL interactive terminal to check connectivity over the encrypted channel (please use real cluster-name, PostgreSQL user login and password):</p> <pre><code>$ kubectl exec -it deployment/pg-client -- bash -il\n[postgres@pg-client /]$ PGSSLMODE=verify-ca PGSSLROOTCERT=/tmp/tls/ca.crt psql postgres://&lt;postgresql-user&gt;:&lt;postgresql-password&gt;@&lt;cluster-name&gt;-pgbouncer.&lt;namespace&gt;.svc.cluster.local\n</code></pre> <p>Now you should see the prompt of PostgreSQL interactive terminal:</p> <pre><code>$ psql ({{ postgresrecommended }})\nType \"help\" for help.\npgdb=&gt;\n</code></pre>"},{"location":"TLS.html#run-percona-distribution-for-postgresql-without-tls","title":"Run Percona Distribution for PostgreSQL without TLS","text":"<p>Omitting TLS is also possible, but we recommend that you run your cluster with the TLS protocol enabled.</p> <p>To disable TLS protocol (e.g. for demonstration purposes) set the <code>spec.tlsOnly</code> key to <code>false</code>, and make sure that there are no certificate secrets configured in the <code>deploy/cr.yaml</code> file.</p>"},{"location":"TLS.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"architecture.html","title":"Design overview","text":"<p>The Percona Operator for PostgreSQL automates and simplifies deploying and managing open source PostgreSQL clusters on Kubernetes. The Operator is based on CrunchyData\u2019s PostgreSQL Operator.</p> <p></p> <p>PostgreSQL containers deployed with the Operator include the following components:</p> <ul> <li> <p>The PostgreSQL database management system, including:</p> <ul> <li> <p>PostgreSQL Additional Supplied Modules,</p> </li> <li> <p>pgAudit PostgreSQL auditing extension,</p> </li> <li> <p>PostgreSQL set_user Extension Module,</p> </li> <li> <p>wal2json output plugin,</p> </li> </ul> </li> <li> <p>The pgBackRest Backup &amp; Restore utility,</p> </li> <li> <p>The pgBouncer connection pooler for PostgreSQL,</p> </li> <li> <p>The PostgreSQL high-availability implementation based on the Patroni template,</p> </li> <li> <p>the pg_stat_monitor PostgreSQL Query Performance Monitoring utility,</p> </li> <li> <p>LLVM (for JIT compilation).</p> </li> </ul> <p>To provide high availability the Operator involves node affinity to run PostgreSQL Cluster instances on separate worker nodes if possible. If some node fails, the Pod with it is automatically re-created on another node.</p> <p></p> <p>To provide data storage for stateful applications, Kubernetes uses Persistent Volumes. A PersistentVolumeClaim (PVC) is used to implement the automatic storage provisioning to pods. If a failure occurs, the Container Storage Interface (CSI) should be able to re-mount storage on a different node.</p> <p>The Operator functionality extends the Kubernetes API with Custom Resources Definitions. These CRDs provide extensions to the Kubernetes API, and, in the case of the Operator, allow you to perform actions such as creating a PostgreSQL Cluster, updating PostgreSQL Cluster resource allocations, adding additional utilities to a PostgreSQL cluster, e.g. pgBouncer for connection pooling and more.</p> <p>When a new Custom Resource is created or an existing one undergoes some changes or deletion, the Operator automatically creates/changes/deletes all needed Kubernetes objects with the appropriate settings to provide a proper Percona PostgreSQL Cluster operation.</p> <p>Following CRDs are created while the Operator installation:</p> <ul> <li> <p><code>pgclusters</code> stores information required to manage a PostgreSQL cluster. This includes things like the cluster name, what storage and resource classes to use, which version of PostgreSQL to run, information about how to maintain a high-availability cluster, etc.</p> </li> <li> <p><code>pgreplicas</code> stores information required to manage the replicas within a PostgreSQL cluster. This includes things like the number of replicas, what storage and resource classes to use, special affinity rules, etc.</p> </li> <li> <p><code>pgtasks</code> is a general purpose CRD that accepts a type of task that is needed to run against a cluster (e.g. take a backup) and tracks the state of said task through its workflow.</p> </li> </ul>"},{"location":"architecture.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"backups.html","title":"Providing Backups","text":"<p>The Operator allows doing backups in two ways. Scheduled backups are configured in the deploy/cr.yaml file to be executed automatically in proper time. On-demand backups can be done manually at any moment.</p> <p>The Operator uses the open source pgBackRest backup and restore utility. A special pgBackRest repository is created by the Operator along with creating a new PostgreSQL cluster to facilitate the usage of the pgBackRest features in it.</p> <p>The Operator can store PostgreSQL backups on Amazon S3, any S3-compatible storage and Google Cloud Storage outside the Kubernetes cluster. Storing backups on Persistent Volume attached to the pgBackRest Pod is also possible. At PostgreSQL cluster creation time, you can specify a specific Storage Class for the pgBackRest repository. Additionally, you can also specify the type of the pgBackRest repository that can be used for backups:</p> <ul> <li><code>local</code>: Uses the storage that is provided by the Kubernetes cluster\u2019s  Storage Class that you select (for historical reasons this repository type can be alternatively named <code>posix</code>),</li> <li><code>s3</code>: Use Amazon S3 or an object storage system that uses the S3 protocol,</li> <li><code>local,s3</code>: Use both the storage that is provided by the Kubernetes cluster\u2019s Storage Class that you select AND Amazon S3 (or equivalent object storage system that uses the S3 protocol).</li> <li><code>gcs</code>: Use Google Cloud Storage,</li> <li><code>local,gcs</code>: Use both the storage that is provided by the Kubernetes cluster\u2019s Storage Class that you select AND Google Cloud Storage.</li> </ul> <p>The pgBackRest repository consists of the following Kubernetes objects:</p> <ul> <li>A Deployment,</li> <li>A Secret that contains information that is specific to the PostgreSQL cluster that it is deployed with (e.g. SSH keys, AWS S3 keys, etc.),</li> <li>A Pod with a number of supporting scripts,</li> <li>A Service.</li> </ul> <p>The PostgreSQL primary is automatically configured to use the <code>pgbackrest archive-push</code> and push the write-ahead log (WAL) archives to the correct repository.</p> <p>The PostgreSQL Operator supports three types of pgBackRest backups:</p> <ul> <li>Full (<code>full</code>): A full backup of all the contents of the PostgreSQL cluster,</li> <li>Differential (<code>diff</code>): A backup of only the files that have changed since the last full backup,</li> <li>Incremental (<code>incr</code>): A backup of only the files that have changed since the last full or differential backup. Incremental backup is the default choice.</li> </ul> <p>The Operator also supports setting pgBackRest retention policies for backups. Backup retention can be controlled by the following pgBackRest options:</p> <ul> <li><code>--repo1-retention-full</code> the number of full backups to retain,</li> <li><code>--repo1-retention-diff</code> the number of differential backups to retain,</li> <li><code>--repo1-retention-archive</code> how many sets of write-ahead log archives to retain alongside the full and differential backups that are retained.</li> </ul> <p>You can set both backups type and retention policy when Making on-demand backup.</p> <p>Also you should first configure the backup storage in the <code>deploy/cr.yaml</code> configuration file to have backups enabled.</p>"},{"location":"backups.html#configuring-the-s3-compatible-backup-storage","title":"Configuring the S3-compatible backup storage","text":"<p>In order to use S3-compatible storage for backups you need to provide some S3-related information, such as proper S3 bucket name, endpoint, etc. This information can be passed to pgBackRest via the following <code>deploy/cr.yaml</code> options in the <code>backup.storages</code> subsection:</p> <ul> <li><code>bucket</code> specifies the AWS S3 bucket that should be utilized, for example <code>my-postgresql-backups-example</code>,</li> <li><code>endpointUrl</code> specifies the S3 endpoint that should be utilized, for example <code>s3.amazonaws.com</code>,</li> <li><code>region</code> specifies the AWS S3 region that should be utilized, for example <code>us-east-1</code>,</li> <li><code>uriStyle</code> specifies whether <code>host</code> or <code>path</code> style URIs should be utilized,</li> <li><code>verifyTLS</code> should be set to <code>true</code> to enable TLS verification or set to <code>false</code> to disable it,</li> <li><code>type</code> should be set to <code>s3</code>.</li> </ul> <p>Here is an example which configures Amazon S3 storage for backups:</p> <pre><code>...\n  backup:\n    ...\n    storages:\n      s3-us-west:\n        type: s3\n        bucket: S3-BACKUP-BUCKET-NAME-HERE\n        region: us-west-2\n  ...\n</code></pre> <p>You also need to supply pgBackRest with base64-encoded AWS S3 key and AWS S3 key secret stored along with other sensitive information in Kubernetes Secrets, e.g. encoding needed data with the following command:</p> in Linuxin macOS <pre><code>$ echo -n 'plain-text-string' | base64 --wrap=0\n</code></pre> <pre><code>$ echo -n 'plain-text-string' | base64\n</code></pre> <p>Edit the <code>deploy/backup/cluster1-backrest-repo-config-secret.yaml</code> configuration file: set there proper cluster name, AWS S3 key, and key secret:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;cluster-name&gt;-backrest-repo-config\ntype: Opaque\ndata:\n  aws-s3-key: &lt;base64-encoded-AWS-S3-key&gt;\n  aws-s3-key-secret: &lt;base64-encoded-AWS-S3-key-secret&gt;\n</code></pre> <p>When done, create the secret as follows:</p> <pre><code>$ kubectl apply -f deploy/backup/cluster1-backrest-repo-config-secret.yaml\n</code></pre> <p>Finally, create or update the cluster:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre>"},{"location":"backups.html#use-google-cloud-storage-for-backups","title":"Use Google Cloud Storage for backups","text":"<p>You can configure Google Cloud Storage as an object store for backups similarly to S3 storage.</p> <p>In order to use Google Cloud Storage (GCS) for backups you need to provide some GCS-related information, such as a proper GCS bucket name. This information can be passed to <code>pgBackRest</code> via the following options in the <code>backup.storages</code> subsection of the <code>deploy/cr.yaml</code> configuration file:</p> <ul> <li><code>bucket</code> should contain the proper bucket name,</li> <li><code>type</code> should be set to <code>gcs</code>.</li> </ul> <p>The Operator will also need your service account key to access storage.</p> <ol> <li>Create your service account key following the official Google Cloud instructions.</li> <li> <p>Export this key from your Google Cloud account.</p> <p>You can find your key in the Google Cloud console (select IAM &amp; Admin \u2192 Service Accounts in the left menu panel, then click your account and open the KEYS tab):</p> <p></p> <p>Click the ADD KEY button, chose Create new key and chose JSON as a key type. These actions will result in downloading a file in JSON format with your new private key and related information.</p> </li> <li> <p>Now you should use a base64-encoded version of this file and to create the Kubernetes Secret. You can encode     the file with the <code>base64 &lt;filename&gt;</code> command. When done, create the     following yaml file with your cluster name and base64-encoded file contents:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;cluster-name&gt;-backrest-repo-config\ntype: Opaque\ndata:\n  gcs-key: &lt;base64-encoded-json-file-contents&gt;\n</code></pre> <p>When done, create the secret as follows:</p> <pre><code>$ kubectl apply -f ./my-gcs-account-secret.yaml\n</code></pre> </li> <li> <p>Finally, create or update the cluster:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre> </li> </ol>"},{"location":"backups.html#scheduling-backups","title":"Scheduling backups","text":"<p>Backups schedule is defined in the <code>backup</code> section of the deploy/cr.yaml file. This section contains following subsections:</p> <ul> <li><code>storages</code> subsection contains data needed to access the S3-compatible cloud to store backups.</li> <li><code>schedule</code> subsection allows to actually schedule backups (the schedule is specified in crontab format).</li> </ul> <p>Here is an example of deploy/cr.yaml which uses Amazon S3 storage for backups:</p> <pre><code>...\nbackup:\n  ...\n  schedule:\n   - name: \"sat-night-backup\"\n     schedule: \"0 0 * * 6\"\n     keep: 3\n     type: full\n     storage: s3\n  ...\n</code></pre> <p>The schedule is specified in crontab format as explained in Custom Resource options.</p>"},{"location":"backups.html#making-on-demand-backup","title":"Making on-demand backup","text":"<p>To make an on-demand backup, the user should use a backup configuration file. The example of the backup configuration file is deploy/backup/backup.yaml.</p> <p>The following keys are most important in the parameters section of this file:</p> <ul> <li><code>parameters.backrest-opts</code> is the string with command line options which will be passed to pgBackRest, for example <code>--type=full --repo1-retention-full=5</code>,</li> <li><code>parameters.pg-cluster</code> is the name of the PostgreSQL cluster to back up, for example <code>cluster1</code>.</li> </ul> <p>When the backup options are configured, execute the actual backup command:</p> <pre><code>$ kubectl apply -f deploy/backup/backup.yaml\n</code></pre>"},{"location":"backups.html#list-existing-backups","title":"List existing backups","text":"<p>To get list of all existing backups in the pgBackrest repo, use the following command:</p> <pre><code>$ kubectl exec &lt;name-of-backrest-shared-repo-pod&gt;  -it -- pgbackrest info\n</code></pre> <p>You can find out the appropriate Pod name using the `` kubectl get pods`` command, as usual. Here is an example of the backups list:</p> <pre><code>$ kubectl exec cluster1-backrest-shared-repo-5ffc465b85-gvhlh -it -- pgbackrest info\nstanza: db\n    status: ok\n    cipher: none\n\n    db (current)\n        wal archive min/max (14): 000000010000000000000001/000000010000000000000003\n\n        full backup: 20220614-104859F\n            timestamp start/stop: 2022-06-14 10:48:59 / 2022-06-14 10:49:13\n            wal start/stop: 000000010000000000000002 / 000000010000000000000002\n            database size: 33.5MB, database backup size: 33.5MB\n            repo1: backup set size: 4.3MB, backup size: 4.3MB\n</code></pre> <p>In this example there is only one backup named <code>20220614-104859F</code>.</p>"},{"location":"backups.html#restore-the-cluster-from-a-previously-saved-backup","title":"Restore the cluster from a previously saved backup","text":"<p>The Operator supports the ability to perform a full restore on a PostgreSQL cluster as well as a point-in-time-recovery. There are two types of ways to restore a cluster:</p> <ul> <li>restore to a new cluster using the pgDataSource.restoreFrom option (and possibly, pgDataSource.restoreOpts for custom pgBackRest options),</li> <li>restore in-place, to an existing cluster (note that this is destructive).</li> </ul> <p>Restoring to a new PostgreSQL cluster allows you to take a backup and create a new PostgreSQL cluster that can run alongside an existing one. There are several scenarios where using this technique is helpful:</p> <ul> <li>Creating a copy of a PostgreSQL cluster that can be used for other purposes. Another way of putting this is creating a clone.</li> <li>Restore to a point-in-time and inspect the state of the data without affecting the current cluster.</li> </ul> <p>To restore the previously saved backup the user should use a backup restore configuration file. The example of the backup configuration file is deploy/backup/restore.yaml:</p> <pre><code>apiVersion: pg.percona.com/v1\nkind: Pgtask\nmetadata:\n  labels:\n    pg-cluster: cluster1\n    pgouser: admin\n  name: cluster1-backrest-restore\n  namespace: pgo\nspec:\n  name: cluster1-backrest-restore\n  namespace: pgo\n  parameters:\n    backrest-restore-from-cluster: cluster1\n    backrest-restore-opts: --type=time --target=\"2021-04-16 15:13:32+00\"\n    backrest-storage-type: local\n  tasktype: restore\n</code></pre> <p>The following keys are the most important in the parameters section of this file:</p> <ul> <li><code>parameters.backrest-restore-cluster</code> specifies the name of a PostgreSQL cluster which will be restored (this option had name <code>parameters.backrest-restore-from-cluster</code> before the Operator 1.2.0). It includes stopping the database and recreating a new primary with the restored data (for example, <code>cluster1</code>),</li> <li><code>parameters.backrest-restore-opts</code> passes through additional options for pgBackRest,</li> <li><code>parameters.backrest-storage-type</code> the type of the pgBackRest repository, (for example, <code>local</code>).</li> </ul> <p>The actual restoration process can be started as follows:</p> <pre><code>$ kubectl apply -f deploy/backup/restore.yaml\n</code></pre> <p>To create a new PostgreSQL cluster from either the active  one, or a former cluster whose pgBackRest repository still exists,  use the pgDataSource.restoreFrom option.</p> <p>The following example will create a new cluster named <code>cluster2</code> from an existing one named``cluster1``.</p> <ol> <li> <p>First, create the <code>cluster2-config-secrets.yaml</code> configuration file with the following content:</p> <pre><code>apiVersion: v1\ndata:\n  password: &lt;base64-encoded-password-for-pguser&gt;\n  username: &lt;base64-encoded-pguser-user-name&gt;\nkind: Secret\nmetadata:\n  labels:\n    pg-cluster: cluster2\n    vendor: crunchydata\n  name: cluster2-pguser-secret\ntype: Opaque\n---\napiVersion: v1\ndata:\n  password: &lt;base64-encoded-password-for-primaryuser&gt;\n  username: &lt;base64-encoded-primaryuser-user-name&gt;\nkind: Secret\nmetadata:\n  labels:\n    pg-cluster: cluster2\n    vendor: crunchydata\n  name: cluster2-primaryuser-secret\ntype: Opaque\n---\napiVersion: v1\ndata:\n  password: &lt;base64-encoded-password-for-postgres-user&gt;\n  username: &lt;base64-encoded-pguser-postgres-name&gt;\nkind: Secret\nmetadata:\n  labels:\n    pg-cluster: cluster2\n    vendor: crunchydata\n  name: cluster2-postgres-secret\ntype: Opaque\n</code></pre> </li> <li> <p>When done, create the secrets as follows:</p> <pre><code>$ kubectl apply -f ./cluster2-config-secrets.yaml\n</code></pre> </li> <li> <p>Edit the <code>deploy/cr.yaml</code> configuration file:</p> <ul> <li>set a new cluster name (<code>cluster2</code>),</li> <li>set the option pgDataSource.restoreFrom to <code>cluster1</code>.</li> </ul> </li> <li> <p>Create the cluster as follows:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre> </li> </ol>"},{"location":"backups.html#restore-the-cluster-with-point-in-time-recovery","title":"Restore the cluster with point-in-time recovery","text":"<p>Point-in-time recovery functionality allows users to revert the database back to a state before an unwanted change had occurred.</p> <p>You can set up a point-in-time recovery using the normal restore command of pgBackRest with few additional options specified in the <code>parameters.backrest-restore-opts</code> key in the backup restore configuration file:</p> <pre><code>...\nspec:\n  name: cluster1-backrest-restore\n  namespace: pgo\n  parameters:\n    backrest-restore-from-cluster: cluster1\n    backrest-restore-opts: --type=time --target=\"2021-04-16 15:13:32+00\"\n</code></pre> <ul> <li>set <code>--type</code> option to <code>time</code>,</li> <li>set <code>--target</code> to a specific time you would like to restore to. You can use the typical string formatted as <code>&lt;YYYY-MM-DD HH:MM:DD&gt;</code>, optionally followed by a timezone offset: <code>\"2021-04-16 15:13:32+00\"</code> (<code>+00</code> in the above example means just UTC),</li> <li>optional <code>--set</code> argument allows you to choose the backup which will be the starting point for point-in-time recovery (look through the available backups to find out the proper backup name). This option must be specified if the target is one or more backups away from the current moment.</li> </ul> <p>After setting these options in the backup restore configuration file, follow the standard restore instructions.</p> <p>Note</p> <p>Make sure you have a backup that is older than your desired point in time. You obviously can\u2019t restore from a time where you do not have a backup. All relevant write-ahead log files must be successfully pushed before you make the restore.</p>"},{"location":"backups.html#delete-a-previously-saved-backup","title":"Delete a previously saved backup","text":"<p>The maximum amount of stored backups is controlled by the backup.schedule.keep option (only successful backups are counted). Older backups are automatically deleted, so that amount of stored backups do not exceed this number.</p> <p>If you want to delete some backup manually, you need to delete both the <code>pgtask</code> object and the corresponding job itself. Deletion of the backup object can be done using the same YAML file which was used for the on-demand backup:</p> <pre><code>$ kubectl delete -f deploy/backup/backup.yaml\n</code></pre> <p>Deletion of the job which corresponds to the backup can be done using <code>kubectl delete jobs</code> command with the backup name:</p> <pre><code>$ kubectl delete jobs cluster1-backrest-full-backup\n</code></pre>"},{"location":"backups.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"cluster-wide.html","title":"Percona Operator for PostgreSQL single-namespace and multi-namespace deployment","text":"<p>There are two design patterns that you can choose from when deploying Percona Operator for PostgreSQL and PostgreSQL clusters in Kubernetes:</p> <ul> <li> <p>Namespace-scope - one Operator per Kubernetes namespace,</p> </li> <li> <p>Cluster-wide - one Operator can manage clusters in multiple namespaces.</p> </li> </ul> <p>This how-to explains how to configure Percona Operator for PostgreSQL for each scenario.</p>"},{"location":"cluster-wide.html#namespace-scope","title":"Namespace-scope","text":"<p>By default, Percona Operator for PostgreSQL functions in a specific Kubernetes namespace. You can create default <code>pgo</code> one or some other Namespace during installation (like it is shown in the installation instructions). This approach allows several Operators to co-exist in one Kubernetes-based environment, being separated in different namespaces:</p> <p></p> <p>Normally this is a recommended approach, as isolation minimizes impact in case of various failure scenarios. This is the default configuration of our Operator.</p> <p>Let\u2019s say you have a Namespace in your Kubernetes cluster called <code>percona-db-1</code>.</p> <ol> <li> <p>Edit the following lines in your deploy/operator.yaml:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: pgo-deployer-cm\ndata:\n  values.yaml: |-\n  ...\n    namespace: \"percona-db-1\"\n    pgo_operator_namespace: \"percona-db-1\"\n\n...\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: pgo-deployer-crb\nsubjects:\n...\n\n  - kind: ServiceAccount\n    namespace: percona-db-1\n</code></pre> </li> <li> <p>Deploy the Operator:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml -n percona-db-1\n</code></pre> </li> <li> <p>Once Operator is up and running, deploy the database cluster itself:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml -n percona-db-1\n</code></pre> </li> </ol> <p>You can deploy multiple clusters in this namespace.</p>"},{"location":"cluster-wide.html#add-more-namespaces","title":"Add more namespaces","text":"<p>What if there is a need to deploy clusters in another namespace? The solution for namespace-scope deployment is to have more than one Operator in the corresponding namespace. We will use the <code>percona-db-2</code> namespace as an example.</p> <ol> <li> <p>Edit or copy <code>operator.yaml</code>:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pgo-deployer-cm\ndata:\n  values.yaml: |-\n...\n     namespace: \"percona-db-2\"\n     pgo_operator_namespace: \"percona-db-2\"\n\n...\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: pgo-deployer-crb\nsubjects:\n...\n   - kind: ServiceAccount\n    namespace: percona-db-2\n</code></pre> </li> <li> <p>Deploy the Operator:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml -n percona-db-2\n</code></pre> </li> <li> <p>Once Operator is up and running deploy the database cluster itself:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml -n percona-db-2\n</code></pre> <p>Note</p> <p>Cluster names may be the same in different namespaces.</p> </li> </ol>"},{"location":"cluster-wide.html#install-the-operator-cluster-wide","title":"Install the Operator cluster-wide","text":"<p>Sometimes it is more convenient to have one Operator watching for Percona Distribution for PostgreSQL custom resources in several namespaces.</p> <p>We recommend running Percona Operator for PostgreSQL in a traditional way, limited to a specific namespace. But it is possible to run it in so-called cluster-wide mode, one Operator watching several namespaces, if needed:</p> <p></p> <p>Note</p> <p>Please take into account that if several Operators are configured to watch the same namespace, it is entirely unpredictable which one will get ownership of the Custom Resource in it, so this situation should be avoided.</p> <p>The following simple example shows how to install Operator cluster-wide on Kubernetes. It does the following:</p> <ul> <li> <p>deploys Operator into a separate <code>percona-operator</code> Namespace,</p> </li> <li> <p>allows Operator to control databases in two Namespaces: <code>percona-db-1</code> and <code>percona-db-2</code>.</p> </li> <li> <p>Edit the following lines in your deploy/operator.yaml:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pgo-deployer-cm\ndata:\n  values.yaml: |-\n  ...\n    namespace: \"percona-db-1,percona-db-2\"\n    pgo_operator_namespace: \"percona-operator\"\n\n...\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: pgo-deployer-crb\nsubjects:\n...\n  - kind: ServiceAccount\n    namespace: percona-operator\n</code></pre> <p>Note</p> <p>Before deploying the Operator, please ensure that all Namespaces exist.</p> </li> <li> <p>Deploy the Operator:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml -n percona-operator\n</code></pre> </li> <li> <p>You can now deploy databases into the namespaces listed in the <code>namespace:</code> variable.</p> <pre><code>$ kubectl apply -f deploy/cr.yaml -n percona-db-1\n$ kubectl apply -f deploy/cr.yaml -n percona-db-2\n</code></pre> </li> </ul>"},{"location":"cluster-wide.html#add-more-namespaces_1","title":"Add more namespaces","text":"<p>Let\u2019s say we want the Operator to manage databases in one more Namespace: <code>percona-db-3</code>.</p> <ol> <li> <p>Edit the <code>operator.yaml</code> and add one more Namespace into the corresponding field:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pgo-deployer-cm\ndata:\n  values.yaml: |-\n  ...\n    namespace: \"percona-db-1,percona-db-2,percona-db-3\"\n</code></pre> </li> <li> <p>Delete the Operator deployment and deploy job:</p> <pre><code>$ kubectl -n percona-operator delete -f deploy/operator.yaml\n$ kubectl -n percona-operator delete deploy postgres-operator\n</code></pre> <p>Note</p> <p>Deletion of the Operator does not affect your existing clusters\u2019 availability, but limits your ability to manage them. For example, you will not be able to scale the clusters or take backups.</p> </li> <li> <p>Deploy the Operator again with the new Namespace added:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml -n percona-operator\n</code></pre> </li> <li> <p>You can now deploy databases into the new Namespace:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml -n percona-db-3\n</code></pre> </li> </ol>"},{"location":"cluster-wide.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"compare.html","title":"Compare various solutions to deploy PostgreSQL in Kubernetes","text":"<p>There are multiple ways to deploy and manage PostgreSQL in Kubernetes. Here we will focus on comparing the following open source solutions:</p> <ul> <li>Crunchy Data PostgreSQL Operator (PGO)</li> <li>CloudNative PG from Enterprise DB </li> <li>Stackgres from OnGres</li> <li>Zalando Postgres Operator</li> <li>Percona Operator for PostgreSQL</li> </ul>"},{"location":"compare.html#generic","title":"Generic","text":"Feature/Product Percona Operator for PostgreSQL Stackgres CrunchyData CloudNativePG (EDB) Zalando Open-source license Apache 2.0 AGPL 3 Apache 2.0, but images are under Developer Program Apache 2.0 MIT PostgreSQL versions 12, 13, 14 14 12, 13, 14 11 - 14, 15 in Beta 11 - 14 Kubernetes conformance Various versions are tested Various versions are tested Various versions are tested Various versions are tested AWS EKS"},{"location":"compare.html#maintenance","title":"Maintenance","text":"Feature/Product Percona Operator for PostgreSQL Stackgres CrunchyData CloudNativePG (EDB) Zalando Operator upgrade Database upgrade Automated and safe Automated and safe Manual Manual Manual Compute scaling Horizontal and vertical Horizontal and vertical Horizontal and vertical Horizontal and vertical Horizontal and vertical Storage scaling Manual Manual Manual Manual Manual, automated for AWS EBS"},{"location":"compare.html#postgresql-topologies","title":"PostgreSQL topologies","text":"Feature/Product Percona Operator for PostgreSQL Stackgres CrunchyData CloudNativePG (EDB) Zalando Warm standby Hot standby Connection pooling Delayed replica Tablespaces"},{"location":"compare.html#backups","title":"Backups","text":"Feature/Product Percona Operator for PostgreSQL Stackgres CrunchyData CloudNativePG (EDB) Zalando Scheduled backups WAL archiving PITR GCS S3 Azure"},{"location":"compare.html#monitoring","title":"Monitoring","text":"Feature/Product Percona Operator for PostgreSQL Stackgres CrunchyData CloudNativePG (EDB) Zalando Solution Percona Monitoring and Management Exposing metrics in Prometheus format Prometheus stack and pgMonitor Exposing metrics in Prometheus format Sidecars"},{"location":"compare.html#miscellaneous","title":"Miscellaneous","text":"Feature/Product Percona Operator for PostgreSQL Stackgres CrunchyData CloudNativePG (EDB) Zalando Customize PostgreSQL configuration Helm Transport encryption Data-at-rest encryption Through storage class Through storage class Through storage class Through storage class Through storage class Create users/roles limited limited"},{"location":"compare.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"constraints.html","title":"Binding Percona Distribution for PostgreSQL components to Specific Kubernetes/OpenShift Nodes","text":"<p>The operator does good job automatically assigning new Pods to nodes with sufficient resources to achieve balanced distribution across the cluster. Still there are situations when it is worth to ensure that pods will land on specific nodes: for example, to get speed advantages of the SSD equipped machine, or to reduce network costs choosing nodes in a same availability zone.</p> <p>Appropriate sections of the deploy/cr.yaml file (such as <code>pgPrimary</code> or <code>pgReplicas</code>) contain keys which can be used to do this, depending on what is the best for a particular situation.</p>"},{"location":"constraints.html#affinity-and-anti-affinity","title":"Affinity and anti-affinity","text":"<p>Affinity makes Pod eligible (or not eligible - so called \u201canti-affinity\u201d) to be scheduled on the node which already has Pods with specific labels, or has specific labels itself (so called \u201cNode affinity\u201d). Particularly, Pod anti-affinity is good to reduce costs making sure several Pods with intensive data exchange will occupy the same availability zone or even the same node - or, on the contrary, to make them land on different nodes or even different availability zones for the high availability and balancing purposes. Node affinity is useful to assign PostgreSQL instances to specific Kubernetes Nodes (ones with specific hardware, zone, etc.).</p> <p>Pod anti-affinity is controlled by the <code>antiAffinityType</code> option, which can be put into <code>pgPrimary</code>, <code>pgBouncer</code>, and <code>backup</code> sections of the <code>deploy/cr.yaml</code> configuration file. This option can be set to one of two values:</p> <ul> <li><code>preferred</code> Pod anti-affinity is a sort of a soft rule. It makes   Kubernetes trying to schedule Pods matching the anti-affinity rules to   different Nodes. If it is not possible, then one or more Pods are scheduled   to the same Node. This variant is used by default.</li> <li><code>required</code> Pod anti-affinity is a sort of a hard rule. It forces   Kubernetes to schedule each Pod matching the anti-affinity rules to different   Nodes. If it is not possible, then a Pod will not be scheduled at all.</li> </ul> <p>Node affinity can be controlled by the <code>pgPrimary.affinity.nodeAffinityType</code> option in the <code>deploy/cr.yaml</code> configuration file. This option can be set to either <code>preferred</code> or <code>required</code> similarly to the <code>antiAffinityType</code> option.</p>"},{"location":"constraints.html#simple-approach-configure-node-affinity-based-on-nodelabel","title":"Simple approach - configure Node Affinity based on nodeLabel","text":"<p>The Operator provides the <code>pgPrimary.affinity.nodeLabel</code> option, which should contains one or more key-value pairs. If the node is not labeled with each key-value pair and <code>nodeAffinityType</code> is set to <code>required</code>, the Pod will not be able to land on it.</p> <p>The following example forces Operator to lend Percona Distribution for PostgreSQL instances on the Nodes having the <code>kubernetes.io/region: us-central1</code> label:</p> <pre><code>affinity:\n  nodeAffinityType: required\n  nodeLabel:\n    kubernetes.io/region: us-central1\n</code></pre>"},{"location":"constraints.html#advanced-approach-use-standard-kubernetes-constraints","title":"Advanced approach - use standard Kubernetes constraints","text":"<p>Previous way can be used with no special knowledge of the Kubernetes way of assigning Pods to specific Nodes. Still in some cases more complex tuning may be needed. In this case <code>pgPrimary.affinity.advanced</code> option placed in the <code>deploy/cr.yaml</code> file turns off the effect of the <code>nodeLabel</code> and allows to use standard Kubernetes affinity constraints of any complexity:</p> <pre><code>affinity:\n   advanced:\n     podAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n       - labelSelector:\n           matchExpressions:\n           - key: security\n             operator: In\n             values:\n             - S1\n         topologyKey: failure-domain.beta.kubernetes.io/zone\n     podAntiAffinity:\n       preferredDuringSchedulingIgnoredDuringExecution:\n       - weight: 100\n         podAffinityTerm:\n           labelSelector:\n             matchExpressions:\n             - key: security\n               operator: In\n               values:\n               - S2\n           topologyKey: kubernetes.io/hostname\n     nodeAffinity:\n       requiredDuringSchedulingIgnoredDuringExecution:\n         nodeSelectorTerms:\n         - matchExpressions:\n           - key: kubernetes.io/e2e-az-name\n             operator: In\n             values:\n             - e2e-az1\n             - e2e-az2\n       preferredDuringSchedulingIgnoredDuringExecution:\n       - weight: 1\n         preference:\n           matchExpressions:\n           - key: another-node-label-key\n             operator: In\n             values:\n             - another-node-label-value\n</code></pre> <p>You can see the explanation of these affinity options in Kubernetes documentation.</p>"},{"location":"constraints.html#default-affinity-rules","title":"Default Affinity rules","text":"<p>The following anti-affinity rules are applied to all Percona Distribution for PostgreSQL Pods:</p> <pre><code>affinity:\n  podAntiAffinity:\n    preferredDuringSchedulingIgnoredDuringExecution:\n    - podAffinityTerm:\n        labelSelector:\n          matchExpressions:\n          - key: vendor\n            operator: In\n            values:\n            - crunchydata\n          - key: pg-pod-anti-affinity\n            operator: Exists\n          - key: pg-cluster\n            operator: In\n            values:\n            - cluster1\n        topologyKey: kubernetes.io/hostname\n      weight: 1\n</code></pre> <p>You can see the explanation of these affinity options in Kubernetes documentation.</p> <p>Note</p> <p>Setting <code>required</code> anti-affinity type will result in placing all Pods on separate nodes, so default configuration will require 7 Kubernetes nodes to deploy the cluster with separate nodes assigned to one PostgreSQL primary, two PostgreSQL replica instances, three pgBouncer and one pgBackrest Pod.</p>"},{"location":"constraints.html#tolerations","title":"Tolerations","text":"<p>Tolerations allow Pods having them to be able to land onto nodes with matching taints. Toleration is expressed as a <code>key</code> with and <code>operator</code>, which is either <code>exists</code> or <code>equal</code> (the latter variant also requires a <code>value</code> the key is equal to). Moreover, toleration should have a specified <code>effect</code>, which may be a self-explanatory <code>NoSchedule</code>, less strict <code>PreferNoSchedule</code>, or <code>NoExecute</code>. The last variant means that if a taint with <code>NoExecute</code> is assigned to node, then any Pod not tolerating this taint will be removed from the node, immediately or after the <code>tolerationSeconds</code> interval, like in the following example.</p> <p>You can use <code>pgPrimary.tolerations</code> key in the <code>deploy/cr.yaml</code> configuration file as follows:</p> <pre><code>tolerations:\n- key: \"node.alpha.kubernetes.io/unreachable\"\n  operator: \"Exists\"\n  effect: \"NoExecute\"\n  tolerationSeconds: 6000\n</code></pre> <p>The Kubernetes Taints and Toleratins contains more examples on this topic.</p>"},{"location":"constraints.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"copyright.html","title":"Copyright and licensing information","text":""},{"location":"copyright.html#documentation-licensing","title":"Documentation licensing","text":"<p>Percona Operator for PostgreSQL documentation is (C)2009-2023 Percona LLC and/or its affiliates and is distributed under the Creative Commons Attribution 4.0 International License.</p>"},{"location":"copyright.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"faq.html","title":"Frequently Asked Questions","text":""},{"location":"faq.html#why-do-we-need-to-follow-the-kubernetes-way-when-kubernetes-was-never-intended-to-run-databases","title":"Why do we need to follow \u201cthe Kubernetes way\u201d when Kubernetes was never intended to run databases?","text":"<p>As it is well known, the Kubernetes approach is targeted at stateless applications but provides ways to store state (in Persistent Volumes, etc.) if the application needs it. Generally, a stateless mode of operation is supposed to provide better safety, sustainability, and scalability, it makes the already-deployed components interchangeable. You can find more about substantial benefits brought by Kubernetes to databases in this blog post.</p> <p>The architecture of state-centric applications (like databases) should be composed in a right way to avoid crashes, data loss, or data inconsistencies during hardware failure. Percona Operator for PostgreSQL provides out-of-the-box functionality to automate provisioning and management of highly available PostgreSQL database clusters on Kubernetes.</p>"},{"location":"faq.html#how-can-i-contact-the-developers","title":"How can I contact the developers?","text":"<p>The best place to discuss Percona Operator for PostgreSQL with developers and other community members is the community forum.</p> <p>If you would like to report a bug, use the Percona Operator for PostgreSQL project in JIRA.</p>"},{"location":"faq.html#how-can-i-analyze-postgresql-logs-with-pgbadger","title":"How can I analyze PostgreSQL logs with pgBadger?","text":"<p>pgBadger is a report generator for PostgreSQL, which can analyze PostgreSQL logs and provide you web-based representation with charts and various statistics. You can configure it via the pgBadger Section in the deploy/cr.yaml file. The most important option there is pgBadger.enabled, which is off by default. When enabled, a separate pgBadger sidecar container with a specialized HTTP server is added to each PostgreSQL Pod.</p> <p>You can generate the log report and access it through an exposed port (10000 by default) and an <code>/api/badgergenerate</code> endpoint: <code>http://&lt;Pod-address&gt;:10000/api/badgergenerate</code>. Also, this report is available in the appropriate pgBadger container as a <code>/report/index.html</code> file.</p>"},{"location":"faq.html#how-can-i-set-the-operator-to-control-postgresql-in-several-namespaces","title":"How can I set the Operator to control PostgreSQL in several namespaces?","text":"<p>Sometimes it is convenient to have one Operator watching for PostgreSQL Cluster custom resources in several namespaces.</p> <p>You can set additional namespace to be watched by the Operator as follows:</p> <ol> <li> <p>First of all clean up the installer artifacts:</p> <pre><code>$ kubectl delete -f deploy/operator.yaml\n</code></pre> </li> <li> <p>Make changes in the <code>deploy/operator.yaml</code> file:</p> <ul> <li> <p>Find the <code>pgo-deployer-cm</code> ConfigMap. It contains the <code>values.yaml</code>     configuration file. Find the <code>namespace</code> key in this file (it is set to     <code>\"pgo\"</code> by default) and append your additional namespace to it in a     comma-separated list.</p> <pre><code>...\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: pgo-deployer-cm\ndata:\n  values.yaml: |-\n    ...\n    namespace: \"pgo,myadditionalnamespace\"\n    ...\n</code></pre> </li> <li> <p>Find the <code>pgo-deploy</code> container template in the <code>pgo-deploy</code> job spec.     It has <code>env</code> element named <code>DEPLOY_ACTION</code>, which you should change     from <code>install</code> to <code>update</code>:</p> <pre><code>...\napiVersion: batch/v1\nkind: Job\nmetadata:\nname: pgo-deploy\n...\n    containers:\n      - name: pgo-deploy\n      ...\n      env:\n        - name: DEPLOY_ACTION\n          value: update\n          ...\n</code></pre> </li> </ul> </li> <li> <p>Now apply your changes as usual:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml\n</code></pre> <p>Note</p> <p>You need to perform cleanup between each <code>DEPLOY_ACTION</code> activity, which can be either <code>install</code>, <code>update</code>, or <code>uninstall</code>.</p> </li> </ol>"},{"location":"faq.html#how-can-i-store-backups-on-s3-compatible-storage-with-self-issued-certificates","title":"How can I store backups on S3-compatible storage with self-issued certificates?","text":"<p>The Operator allows you to store backups on any S3-compatible storage including your private one (for example, a local MinIO installation). Backup and restore with a private S3-compatible storage can be done following the official instruction except the case when you use self-signed certificates and would like to skip TLS verification (which can be reasonable when both your database and storage are located in the same Kubernetes cluster or in the same protected intranet segment).</p> <p>The  backup.storages. option in the <code>deploy/cr.yaml</code> configuration file allows you to skip TLS verification for specific S3-compatible storage. Setting it to <code>true</code> is enough to make a backup.</p> <p>Restoring a backup without TLS requires you to make two changes in the <code>parameters</code> subsection of the <code>deploy/restore.yaml</code> file:</p> <ul> <li>set <code>backrest-s3-verify-tls</code> option to <code>false</code>,</li> <li>add <code>--no-repo1-storage-verify-tls</code> value to <code>backrest-restore-opts</code> field.</li> </ul> <p>The following example shows how the resulting <code>parameters</code> section may look like:</p> <pre><code>...\nparameters:\n backrest-restore-from-cluster: cluster1\n backrest-restore-opts: --type=time --target=\"2022-05-03 15:22:42\" --no-repo1-storage-verify-tls\n backrest-storage-type: \"s3\"\n backrest-s3-verify-tls: \"false\"\ntasktype: restore\n</code></pre>"},{"location":"faq.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"gke.html","title":"Install Percona Distribution for PostgreSQL on Google Kubernetes Engine (GKE)","text":"<p>Following steps will allow you to install the Operator and use it to manage Percona Distribution for PostgreSQL with the Google Kubernetes Engine. The document assumes some experience with Google Kubernetes Engine (GKE). For more information on the GKE, see the Kubernetes Engine Quickstart.</p>"},{"location":"gke.html#prerequisites","title":"Prerequisites","text":"<p>All commands from this quickstart can be run either in the Google Cloud shell or in your local shell.</p> <p>To use Google Cloud shell, you need nothing but a modern web browser.</p> <p>If you would like to use your local shell, install the following:</p> <ol> <li> <p>gcloud. This tool is part of the Google Cloud SDK. To install it, select your operating system on the official Google Cloud SDK documentation page and then follow the instructions.</p> </li> <li> <p>kubectl. It is the Kubernetes command-line tool you will use to manage and deploy applications. To install the tool, run the following command:</p> <pre><code>$ gcloud auth login\n$ gcloud components install kubectl\n</code></pre> </li> </ol>"},{"location":"gke.html#configuring-default-settings-for-the-cluster","title":"Configuring default settings for the cluster","text":"<p>You can configure the settings using the <code>gcloud</code> tool. You can run it either in the Cloud Shell or in your local shell (if you have installed Google Cloud SDK locally on the previous step). The following command will create a cluster named <code>my-cluster-1</code>:</p> <pre><code>$ gcloud container clusters create cluster-1 --project &lt;project name&gt; --zone us-central1-a --cluster-version {{ gkerecommended }} --machine-type n1-standard-4 --num-nodes=3\n</code></pre> <p>Note</p> <p>You must edit the following command and other command-line statements to replace the <code>&lt;project name&gt;</code> placeholder with your project name. You may also be required to edit the zone location, which is set to <code>us-central1</code> in the above example. Other parameters specify that we are creating a cluster with 3 nodes and with machine type of 4 vCPUs and 45 GB memory.</p> <p>You may wait a few minutes for the cluster to be generated, and then you will see it listed in the Google Cloud console (select Kubernetes Engine \u2192 Clusters in the left menu panel):</p> <p></p> <p>Now you should configure the command-line access to your newly created cluster to make <code>kubectl</code> be able to use it.</p> <p>In the Google Cloud Console, select your cluster and then click the Connect shown on the above image. You will see the connect statement configures command-line access. After you have edited the statement, you may run the command in your local shell:</p> <pre><code>$ gcloud container clusters get-credentials cluster-1 --zone us-central1-a --project &lt;project name&gt;\n</code></pre>"},{"location":"gke.html#installing-the-operator","title":"Installing the Operator","text":"<ol> <li> <p>First of all, use your Cloud Identity and Access Management (Cloud IAM) to control access to the cluster. The following command will give you the ability to create Roles and RoleBindings:</p> <pre><code>$ kubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $(gcloud config get-value core/account)\n</code></pre> Expected output <pre><code>clusterrolebinding.rbac.authorization.k8s.io/cluster-admin-binding created\n</code></pre> </li> <li> <p>Use the following <code>git clone</code> command to download the correct branch of the percona-postgresql-operator repository:</p> <pre><code>$ git clone -b v{{ release }} https://github.com/percona/percona-postgresql-operator\n$ cd percona-postgresql-operator\n</code></pre> </li> <li> <p>The next thing to do is to add the <code>pgo</code> namespace to Kubernetes, not forgetting to set the correspondent context for further steps:</p> <pre><code>$ kubectl create namespace pgo\n$ kubectl config set-context $(kubectl config current-context) --namespace=pgo\n</code></pre> <p>Note</p> <p>To use different namespace, you should edit all occurrences of the <code>namespace: pgo</code> line in both <code>deploy/cr.yaml</code> and <code>deploy/operator.yaml</code> configuration files.</p> </li> <li> <p>Deploy the operator with the following command:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml\n</code></pre> Expected output <pre><code>serviceaccount/pgo-deployer-sa created\nclusterrole.rbac.authorization.k8s.io/pgo-deployer-cr created\nconfigmap/pgo-deployer-cm created\nclusterrolebinding.rbac.authorization.k8s.io/pgo-deployer-crb created\njob.batch/pgo-deploy created\n</code></pre> <p>The last line of the command output mentions the <code>pgo-deploy</code> Kubernetes Job created to carry on the Operator deployment process. It can take several minutes to be completed. You can track it with the following command:</p> <pre><code>$ kubectl get job/pgo-deploy\n</code></pre> Expected output <pre><code>NAME         COMPLETIONS   DURATION   AGE\npgo-deploy   1/1           81s        5m53s\n</code></pre> <p>When it reaches the COMPLETIONS count of <code>1/1</code>, you can safely delete the job as follows:</p> <pre><code>$ kubectl delete  job/pgo-deploy\n</code></pre> <p>Note</p> <p>Deleting the <code>pgo-deploy</code> job will be needed before upgrading the Operator.</p> </li> <li> <p>After the operator is started Percona Distribution for PostgreSQL can be created at any time with the following commands:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre> <p>Creation process will take some time. The process is over when the Operator and PostgreSQL Pods have reached their Running status:</p> <pre><code>$ kubectl get pods\n</code></pre> Expected output <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\nbackrest-backup-cluster1-4nq2x                    0/1     Completed 0          10m\ncluster1-6c9d4f9678-qdfx2                         1/1     Running   0          10m\ncluster1-backrest-shared-repo-7cb4dd8f8f-sh5gg    1/1     Running   0          10m\ncluster1-pgbouncer-6cd69d8966-vlxdt               1/1     Running   0          10m\npgo-deploy-bp2ts                                  0/1     Completed 0          5m\npostgres-operator-67f58bcb8c-9p4tl                4/4     Running   1          5m\n</code></pre> <p>Also, you can see the same information when browsing Pods of your cluster in Google Cloud console via the Object Browser:</p> <p></p> </li> <li> <p>During previous steps, the Operator has generated several secrets, including the password for the <code>pguser</code> user, which you will need to access the cluster.</p> <p>Use <code>kubectl get secrets</code> command to see the list of Secrets objects (by default Secrets object you are interested in has <code>cluster1-pguser-secret</code> name). Then you can use <code>kubectl get secret cluster1-pguser-secret -o yaml</code> to look through the YAML file with generated secrets (the actual password will be base64-encoded), or just get the needed password with the following command:</p> <pre><code>$ kubectl get secrets cluster1-users -o yaml -o jsonpath='{.data.pguser}' | base64 --decode | tr '\\n' ' ' &amp;&amp; echo \" \"\n</code></pre> </li> <li> <p>Check connectivity to newly created cluster. Run a new Pod to use it as a client and connect its console output to your terminal (running it may require some time to deploy). When you see the command line prompt of the newly created Pod, run <code>psql</code> tool using the password obtained from the secret. The following command will do this, naming the new Pod <code>pg-client</code>:</p> <pre><code>$ kubectl run -i --rm --tty pg-client --image=perconalab/percona-distribution-postgresql:{{ postgresrecommended }} --restart=Never -- bash -il\n[postgres@pg-client /]$ PGPASSWORD='pguser_password' psql -h cluster1-pgbouncer -p 5432 -U pguser pgdb\n</code></pre> <p>This command will connect you to the PostgreSQL interactive terminal.</p> <pre><code>$ psql ({{ postgresrecommended }})\nType \"help\" for help.\npgdb=&gt;\n</code></pre> </li> </ol>"},{"location":"gke.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"glossary.html","title":"The Operator options glossary","text":""},{"location":"glossary.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"helm.html","title":"Install Percona Distribution for PostgreSQL using Helm","text":"<p>Helm is the package manager for Kubernetes. Percona Helm charts can be found in percona/percona-helm-charts repository in Github.</p>"},{"location":"helm.html#pre-requisites","title":"Pre-requisites","text":"<p>Install Helm following its official installation instructions.</p> <p>Note</p> <p>Helm v3 is needed to run the following steps.</p>"},{"location":"helm.html#installation","title":"Installation","text":"<ol> <li> <p>Add the Percona\u2019s Helm charts repository and make your Helm client up to     date with it:</p> <pre><code>$ helm repo add percona https://percona.github.io/percona-helm-charts/\n$ helm repo update\n</code></pre> </li> <li> <p>Install the Percona Operator for PostgreSQL:</p> <pre><code>$ helm install my-operator percona/pg-operator\n</code></pre> <p>The <code>my-operator</code> parameter in the above example is the name of a new release object which is created for the Operator when you install its Helm chart (use any name you like).</p> <p>Note</p> <p>If nothing explicitly specified, <code>helm install</code> command will work with the <code>default</code> namespace and the latest version of the Helm chart. </p> <ul> <li> <p>To use different namespace, provide its name with     the following additional parameter: <code>--namespace my-namespace</code>.</p> </li> <li> <p>To use different Helm chart version, provide it as follows:     <code>--version {{ release }}</code></p> </li> </ul> </li> <li> <p>Install PostgreSQL:</p> <pre><code>$ helm install my-db percona/pg-db\n</code></pre> <p>The <code>my-db</code> parameter in the above example is the name of a new release object which is created for the Percona Distribution for PostgreSQL when you install its Helm chart (use any name you like).</p> </li> </ol>"},{"location":"helm.html#installing-percona-distribution-for-postgresql-with-customized-parameters","title":"Installing Percona Distribution for PostgreSQL with customized parameters","text":"<p>The command above installs Percona Distribution for PostgreSQL with default parameters. Custom options can be passed to a <code>helm install</code> command as a <code>--set key=value[,key=value]</code> argument. The options passed with a chart can be any of the Operator\u2019s Custom Resource options.</p> <p>The following example will deploy a Percona Distribution for PostgreSQL Cluster in the <code>pgdb</code> namespace, with enabled Percona Monitoring and Management (PMM) and 20 Gi storage for a Primary PostgreSQL node:</p> <pre><code>$ helm install my-db percona/pg-db --version {{ release }} --namespace my-namespace \\\n  --set pgPrimary.volumeSpec.size=20Gi \\\n  --set pmm.enabled=true\n</code></pre>"},{"location":"helm.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"images.html","title":"Percona certified images","text":"<p>Following table presents Percona\u2019s certified docker images to be used with the Percona Operator for PostgreSQL:</p> Image Digest percona/percona-postgresql-operator:1.5.1-pgo-deployer 565366c2ffcfb46a45f461602f6a2ba392524e1de3b82867a77a305c7b6f76db percona/percona-postgresql-operator:1.5.1-postgres-operator 45a100162e7ef4e0b014909d2ca84f4a3d95a8a1d02f8144ab2ed09b7eec4129 percona/percona-postgresql-operator:1.5.1-pgo-scheduler 5f15e10d3c9a69dae0ed6baa4c57924368179659f351eb3cf35e3312fe56155c percona/percona-postgresql-operator:1.5.1-pgo-rmdata b705e755daecac4054f2e07fb8b14c4f316f53f26187ebc4be8d1ed5bd3ffe61 percona/percona-postgresql-operator:1.5.1-pgo-event 141305674261d49a8156a43e6acb1964b41b98e388546b88464839ad97dbd590 percona/percona-postgresql-operator:1.5.1-pgo-apiserver e61b9c2d8f845a01ac7c8ff3537a7e59bad6b518d9d9eb000a9ee1b3aa050169 percona/percona-postgresql-operator:1.5.1-ppg12-pgbadger af0bff618b5f66647c35779163a53854c5457832adedb0926c7990c64ca5c510 percona/percona-postgresql-operator:1.5.1-ppg13-pgbadger 38c705a4a03db98c090b0595f23b9de2cb5c9636e0a149ae0d15950e4a4f763f percona/percona-postgresql-operator:1.5.1-ppg14-pgbadger 74acf90e9471513b0e33867ca4f7b6c4c5df3125a95dbf336523300285d85525 percona/percona-postgresql-operator:1.5.1-ppg12-postgres-ha cf8eded64d1da50f430d5eaeb22c5cd425750e7b361c7b5382d48762094dbfc8 percona/percona-postgresql-operator:1.5.1-ppg13-postgres-ha 9ccbeab54947a33fe681f9f9ad3511cbf446be0cf619887bd4157152c3c207af percona/percona-postgresql-operator:1.5.1-ppg14-postgres-ha bcd7c6edf3890749ce32219079e63cb24dd255daf38cca54f272d01a34b5f53a percona/percona-postgresql-operator:1.5.1-ppg12-pgbouncer c72276b647231380f30185a6943bec583895068eda0219b15f9bad90cf29f9ce percona/percona-postgresql-operator:1.5.1-ppg13-pgbouncer dfad497290026cf5b14af970daf38b1d1b482ce712a4e36e0f2e302994786251 percona/percona-postgresql-operator:1.5.1-ppg14-pgbouncer 019c12422c9a33da1bf59d8065c172d82f097c0153b357b8066bf6c7a5268146 percona/percona-postgresql-operator:1.5.1-ppg12-pgbackrest bbe89ae227b6c8a9afe49e4ba269e412f45da4ef43752a6b9b2083bd6e77c290 percona/percona-postgresql-operator:1.5.1-ppg13-pgbackrest dcf286797f313ef3039a7fdc76dfcc0f17433fd90e81496134e6e25e5f385bd1 percona/percona-postgresql-operator:1.5.1-ppg14-pgbackrest cdb4c7a3251efee0bb851d0ae7e73b0e55b8a7b786b47f2f9e6fc557446643a8 percona/percona-postgresql-operator:1.5.1-ppg12-pgbackrest-repo e107395950e72eb7b93cf82952a0e1a888cebffed443491aafb6b771a1e40f19 percona/percona-postgresql-operator:1.5.1-ppg13-pgbackrest-repo 7147f21a6b43f946cebd584a198ea956137fe71f05ffca8b95b6666d694467eb percona/percona-postgresql-operator:1.5.1-ppg14-pgbackrest-repo f375f40628b4f1f26f11a4a1e5b77f1324d11a78b261ff96dea04887bffeb112 percona/pmm-client:2.40.1 6ebb6f7ef111949bcb83f0e51dec0682e389517a46347a4b1a0402c7aec8efa8"},{"location":"images.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"installation-options.html","title":"The Operator installation options","text":"<p>When installing The Operator, you can customize additional configuration options. These options are specified in <code>deploy/operator.yaml</code> file and already have reasonable defaults, so most users have no need modifying them.</p>"},{"location":"installation-options.html#general-configuration","title":"General Configuration","text":"<p>These variables affect the general configuration of the PostgreSQL Operator.</p> Name Default Required Description archive_mode <code>true</code> If <code>true</code>, enables archive logging on all newly created clusters archive_timeout <code>60</code> Set to a value in seconds to configure the timeout threshold for archiving ccp_image_pull_secret <code>\"\"</code> Name of a Secret with credentials for the container image registries for the PostgreSQL cluster ccp_image_pull_secret_manifest <code>\"\"</code> A path to the Secret manifest to be installed in each namespace (optional) create_rbac <code>true</code> Set to <code>true</code> if the installer should create the RBAC resources required to run the PostgreSQL Operator delete_operator_namespace <code>false</code> If <code>true</code>, the Operator namespace (one defined using the <code>pgo_operator_namespace</code> variable) will be deleted when uninstalling the Operator delete_watched_namespaces <code>false</code> If <code>true</code>, the Operator watched namespaces (ones defined using the <code>namespace</code> variable) will be deleted when uninstalling the Operator disable_telemetry <code>false</code> If <code>true</code>, gathering telemetry by the Operator will be disabled namespace <code>pgo</code> A comma delimited string of all the namespaces the Operator should manage namespace_mode <code>disabled</code> Determines which namespace permissions are assigned to the PostgreSQL Operator using a ClusterRole; can be <code>dynamic</code>, <code>readonly</code>, and <code>disabled</code> pgo_image_prefix <code>percona/percona-postgresql-operator</code> The image prefix used when creating containers for the Operator (apiserver, operator, scheduler, etc.) pgo_image_pull_policy <code>Always</code> The policy for updating the Operator images pgo_image_pull_secret <code>\"\"</code> Name of a Secret with credentials for the Operator\u2019s container image registries pgo_image_pull_secret_manifest <code>\"\"</code> Optionally provides a path to the Secret manifest to be installed in each namespace pgo_image_tag <code>{{ release }}</code> Configures the image tag used when creating the Operator\u2019s containers (apiserver, operator, scheduler, etc.) pgo_operator_namespace <code>pgo</code> The namespace where the Operator will be deployed"},{"location":"installation-options.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"kubernetes.html","title":"Install Percona Distribution for PostgreSQL on Kubernetes","text":"<p>Following steps will allow you to install the Operator and use it to manage Percona Distribution for PostgreSQL in a Kubernetes-based environment.</p> <ol> <li> <p>First of all, clone the percona-postgresql-operator repository:</p> <pre><code>$ git clone -b v{{ release }} https://github.com/percona/percona-postgresql-operator\n$ cd percona-postgresql-operator\n</code></pre> <p>Note</p> <p>It is crucial to specify the right branch with <code>-b</code> option while cloning the code on this step. Please be careful.</p> </li> <li> <p>The next thing to do is to add the <code>pgo</code> namespace to Kubernetes, not forgetting to set the correspondent context for further steps:</p> <pre><code>$ kubectl create namespace pgo\n$ kubectl config set-context $(kubectl config current-context) --namespace=pgo\n</code></pre> <p>Note</p> <p>To use different namespace, you should edit all occurrences of the <code>namespace: pgo</code> line in both <code>deploy/cr.yaml</code> and <code>deploy/operator.yaml</code> configuration files.</p> </li> <li> <p>Deploy the operator with the following command:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml\n</code></pre> Expected output <pre><code>serviceaccount/pgo-deployer-sa created\nclusterrole.rbac.authorization.k8s.io/pgo-deployer-cr created\nconfigmap/pgo-deployer-cm created\nclusterrolebinding.rbac.authorization.k8s.io/pgo-deployer-crb created\njob.batch/pgo-deploy created\n</code></pre> <p>The last line of the command output mentions the <code>pgo-deploy</code> Kubernetes Job created to carry on the Operator deployment process. It can take several minutes to be completed. You can track it with the following command:</p> <pre><code>$ kubectl get job/pgo-deploy\n</code></pre> Expected output <pre><code>NAME         COMPLETIONS   DURATION   AGE\npgo-deploy   1/1           81s        5m53s\n</code></pre> <p>When it reaches the COMPLETIONS count of <code>1/1</code>, you can safely delete the job as follows:</p> <pre><code>$ kubectl delete  job/pgo-deploy\n</code></pre> <p>Note</p> <p>Deleting the <code>pgo-deploy</code> job will be needed before upgrading the Operator.</p> </li> <li> <p>After the operator is started Percona Distribution for PostgreSQL can be created at any time with the following command:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre> <p>Creation process will take some time. The process is over when both operator and replica set pod have reached their Running status:</p> <pre><code>$ kubectl get pods\n</code></pre> Expected output <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\nbackrest-backup-cluster1-j275w                    0/1     Completed 0          10m\ncluster1-85486d645f-gpxzb                         1/1     Running   0          10m\ncluster1-backrest-shared-repo-6495464548-c8wvl    1/1     Running   0          10m\ncluster1-pgbouncer-fc45869f7-s86rf                1/1     Running   0          10m\npgo-deploy-rhv6k                                  0/1     Completed 0          5m\npostgres-operator-8646c68b57-z8m62                4/4     Running   1          5m\n</code></pre> </li> <li> <p>During previous steps, the Operator has generated several secrets, including the password for the <code>pguser</code> user, which you will need to access the cluster.</p> <p>Use <code>kubectl get secrets</code> command to see the list of Secrets objects (by default Secrets object you are interested in has <code>cluster1-pguser-secret</code> name). Then you can use <code>kubectl get secret cluster1-pguser-secret -o yaml</code> to look through the YAML file with generated secrets (the actual password will be base64-encoded), or just get the needed password with the following command:</p> <pre><code>$ kubectl get secrets cluster1-users -o yaml -o jsonpath='{.data.pguser}' | base64 --decode | tr '\\n' ' ' &amp;&amp; echo \" \"\n</code></pre> </li> <li> <p>Check connectivity to newly created cluster. Run a new Pod to use it as a client and connect its console output to your terminal (running it may require some time to deploy). When you see the command line prompt of the newly created Pod, run <code>psql</code> tool using the password obtained from the secret. The following command will do this, naming the new Pod <code>pg-client</code>:</p> <pre><code>$ kubectl run -i --rm --tty pg-client --image=perconalab/percona-distribution-postgresql:{{ postgresrecommended }} --restart=Never -- bash -il\n[postgres@pg-client /]$ PGPASSWORD='pguser_password' psql -h cluster1-pgbouncer -p 5432 -U pguser pgdb\n</code></pre> <p>This command will connect you to the PostgreSQL interactive terminal.</p> <pre><code>$ psql ({{ postgresrecommended }})\nType \"help\" for help.\npgdb=&gt;\n</code></pre> </li> </ol>"},{"location":"kubernetes.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"minikube.html","title":"Install Percona Distribution for PostgreSQL on Minikube","text":"<p>Installing the Percona Operator for PostgreSQL on minikube is the easiest way to try it locally without a cloud provider. Minikube runs Kubernetes on GNU/Linux, Windows, or macOS system using a system-wide hypervisor, such as VirtualBox, KVM/QEMU, VMware Fusion or Hyper-V. Using it is a popular way to test the Kubernetes application locally prior to deploying it on a cloud.</p> <p>The following steps are needed to run Percona Operator for PostgreSQL on minikube:</p> <ol> <li> <p>Install minikube, using a way recommended for your system. This includes the installation of the following three components:</p> <ol> <li>kubectl tool,</li> <li>a hypervisor, if it is not already installed,</li> <li>actual minikube package</li> </ol> <p>After the installation, run <code>minikube start</code> command. Being executed, this command will download needed virtualized images, then initialize and run the cluster. After minikube is successfully started, you can optionally run the Kubernetes dashboard, which visually represents the state of your cluster. Executing <code>minikube dashboard</code> will start the dashboard and open it in your default web browser.</p> </li> <li> <p>The first thing to do is to add the <code>pgo</code> namespace to Kubernetes,     not forgetting to set the correspondent context for further steps:</p> <pre><code>$ kubectl create namespace pgo\n$ kubectl config set-context $(kubectl config current-context) --namespace=pgo\n</code></pre> <p>Note</p> <p>To use different namespace, you should edit all occurrences of the <code>namespace: pgo</code> line in both <code>deploy/cr.yaml</code> and <code>deploy/operator.yaml</code> configuration files.</p> <p>If you use Kubernetes dashboard, choose your newly created namespace to be shown instead of the default one:</p> <p></p> </li> <li> <p>Deploy the operator with the following command:</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/percona/percona-postgresql-operator/v{{ release }}/deploy/operator.yaml\n</code></pre> Expected output <pre><code>serviceaccount/pgo-deployer-sa created\nclusterrole.rbac.authorization.k8s.io/pgo-deployer-cr created\nconfigmap/pgo-deployer-cm created\nclusterrolebinding.rbac.authorization.k8s.io/pgo-deployer-crb created\njob.batch/pgo-deploy created\n</code></pre> <p>The last line of the command output mentions the <code>pgo-deploy</code> Kubernetes Job created to carry on the Operator deployment process. It can take several minutes to be completed. You can track it with the following command:</p> <pre><code>$ kubectl get job/pgo-deploy\n</code></pre> Expected output <pre><code>NAME         COMPLETIONS   DURATION   AGE\npgo-deploy   1/1           81s        5m53s\n</code></pre> <p>When it reaches the COMPLETIONS count of <code>1/1</code>, you can safely delete the job as follows:</p> <pre><code>$ kubectl delete  job/pgo-deploy\n</code></pre> <p>Note</p> <p>Deleting the <code>pgo-deploy</code> job will be needed before upgrading the Operator.</p> </li> <li> <p>Deploy Percona Distribution for PostgreSQL:</p> <pre><code>$ kubectl apply -f https://raw.githubusercontent.com/percona/percona-postgresql-operator/v{{ release }}/deploy/cr-minimal.yaml\n</code></pre> <p>This deploys PostgreSQL on one node, because <code>deploy/cr-minimal.yaml</code> is for minimal non-production deployment. For more configuration options please see <code>deploy/cr.yaml</code> and Custom Resource Options.</p> <p>Creation process will take some time. The process is over when both operator and replica set pod have reached their Running status:</p> <pre><code>$ kubectl get pods\n</code></pre> Expected output <pre><code>NAME                                                    READY   STATUS      RESTARTS   AGE\nbackrest-backup-minimal-cluster-dcvkw                   0/1     Completed   0          68s\nminimal-cluster-6dfd645d94-42xsr                        1/1     Running     0          2m5s\nminimal-cluster-backrest-shared-repo-77bd498dfd-9msvp   1/1     Running     0          2m23s\nminimal-cluster-pgbouncer-594bf56d-kjwrp                1/1     Running     0          84s\npgo-deploy-lnbv7                                        0/1     Completed   0          4m14s\npostgres-operator-6c4c558c5-dkk8v                       4/4     Running     0          3m37s\n</code></pre> <p>You can also track the progress via the Kubernetes dashboard:</p> <p></p> </li> <li> <p>During previous steps, the Operator has generated several secrets,     including the password for the <code>pguser</code> user, which you will need to access     the cluster.</p> <p>Use <code>kubectl get secrets</code> command to see the list of Secrets objects (by default Secrets object you are interested in has <code>minimal-cluster-pguser-secret</code> name). Then you can use <code>kubectl get secret minimal-cluster-pguser-secret -o yaml</code> to look through the YAML file with generated secrets (the actual password will be base64-encoded), or just get the needed password with the following command:</p> <pre><code>$ kubectl get secrets minimal-cluster-users -o yaml -o jsonpath='{.data.pguser}' | base64 --decode | tr '\\n' ' ' &amp;&amp; echo \" \"\n</code></pre> </li> <li> <p>Check connectivity to a newly created cluster.</p> <p>Run new Pod to use it as a client and connect its console output to your terminal (running it may require some time to deploy). When you see the command line prompt of the newly created Pod, run <code>psql</code> tool using the password obtained from the secret. The following command will do this, naming the new Pod <code>pg-client</code>:</p> <pre><code>$ kubectl run -i --rm --tty pg-client --image=perconalab/percona-distribution-postgresql:{{ postgresrecommended }} --restart=Never -- bash -il\n[postgres@pg-client /]$ PGPASSWORD='pguser_password' psql -h minimal-cluster -p 5432 -U pguser pgdb\n</code></pre> <p>This command will connect you to the  PostgreSQL interactive terminal.</p> <pre><code>$ psql ({{ postgresrecommended }})\nType \"help\" for help.\npgdb=&gt;\n</code></pre> </li> </ol>"},{"location":"minikube.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"monitoring.html","title":"Monitoring","text":"<p>Percona Monitoring and Management (PMM) provides an excellent solution to monitor Percona Distribution for PostgreSQL.</p> <p>Note</p> <p>Only PMM 2.x versions are supported by the Operator.</p> <p>PMM is a client/server application. PMM Client runs on each node with the database you wish to monitor: it collects needed metrics and sends gathered data to PMM Server. As a user, you connect to PMM Server to see database metrics on a number of dashboards.</p> <p>That\u2019s why PMM Server and PMM Client need to be installed separately.</p>"},{"location":"monitoring.html#installing-the-pmm-server","title":"Installing the PMM Server","text":"<p>PMM Server runs as a Docker image, a virtual appliance, or on an AWS instance. Please refer to the official PMM documentation for the installation instructions.</p>"},{"location":"monitoring.html#installing-the-pmm-client","title":"Installing the PMM Client","text":"<p>The following steps are needed for the PMM client installation in your Kubernetes-based environment:</p> <ol> <li> <p>The PMM client installation is initiated by updating the <code>pmm</code>     section in the     deploy/cr.yaml     file.</p> <ul> <li>set <code>pmm.enabled=true</code></li> <li>set the <code>pmm.serverHost</code> key to your PMM Server hostname or IP address     (it should be resolvable and reachable from within your cluster)</li> <li>check that  the <code>pmm.serverUser</code> key contains your PMM Server user name     (<code>admin</code> by default),</li> <li>make sure the <code>password</code> key in the       deploy/pmm-secret.yaml     secrets file contains the password specified for the PMM Server during its     installation.</li> </ul> <p>Apply changes with the <code>kubectl apply -f deploy/pmm-secret.yaml</code> command.</p> <p>Info</p> <p>You use <code>deploy/pmm-secret.yaml</code> file to create Secrets Object. The file contains all values for each key/value pair in a convenient plain text format. But the resulting Secrets contain passwords stored as base64-encoded strings. If you want to update password field, you\u2019ll need to encode the value into base64 format. To do this, you can run <code>echo -n \"password\" | base64 --wrap=0</code> (or just <code>echo -n \"password\" | base64</code> in case of Apple macOS) in your local shell to get valid values. For example, setting the PMM Server user\u2019s password to <code>new_password</code> in the <code>cluster1-pmm-secret</code> object can be done with the following command:</p> in Linuxin macOS <pre><code>$ kubectl patch secret/cluster1-pmm-secret -p '{\"data\":{\"password\": '$(echo -n new_password | base64 --wrap=0)'}}'\n</code></pre> <pre><code>$ kubectl patch secret/cluster1-pmm-secret -p '{\"data\":{\"password\": '$(echo -n new_password | base64)'}}'\n</code></pre> <p>When done, apply the edited <code>deploy/cr.yaml</code> file:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre> </li> <li> <p>Check that corresponding Pods are not in a cycle of stopping and restarting.     This cycle occurs if there are errors on the previous steps:</p> <pre><code>$ kubectl get pods\n$ kubectl logs cluster1-7b7f7898d5-7f5pz -c pmm-client\n</code></pre> </li> <li> <p>Now you can access PMM via https in a web browser, with the     login/password authentication, and the browser is configured to show     Percona Distribution for PostgreSQL metrics.</p> </li> </ol>"},{"location":"monitoring.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"openshift.html","title":"Install Percona Distribution for PostgreSQL on OpenShift","text":"<p>Following steps will allow you to install the Operator and use it to manage Percona Distribution for PostgreSQL on Red Hat OpenShift platform. For more information on the OpenShift, see its official documentation.</p> <p>Following steps will allow you to install the Operator and use it to manage Percona Distribution for PostgreSQL on OpenShift.</p> <ol> <li> <p>First of all, clone the percona-postgresql-operator repository:</p> <pre><code>git clone -b v{{ release }} https://github.com/percona/percona-postgresql-operator\ncd percona-postgresql-operator\n</code></pre> <p>Note</p> <p>It is crucial to specify the right branch with <code>-b</code> option while cloning the code on this step. Please be careful.</p> </li> <li> <p>The next thing to do is to add the <code>pgo</code> namespace to Kubernetes,     not forgetting to set the correspondent context for further steps:</p> <pre><code>$ oc create namespace pgo\n$ oc config set-context $(kubectl config current-context) --namespace=pgo\n</code></pre> <p>Note</p> <p>To use different namespace, you should edit all occurrences of the <code>namespace: pgo</code> line in both <code>deploy/cr.yaml</code> and <code>deploy/operator.yaml</code> configuration files.</p> </li> <li> <p>If you are going to use the operator with anyuid https://docs.openshift.com/container-platform/4.9/authentication/managing-security-context-constraints.html security context constraint     please execute the following command:</p> <pre><code>$ sed -i '/disable_auto_failover: \"false\"/a \\ \\ \\ \\ disable_fsgroup: \"false\"' deploy/operator.yaml\n</code></pre> </li> <li> <p>Deploy the operator with the following command:</p> <pre><code>$ oc apply -f deploy/operator.yaml\n</code></pre> </li> <li> <p>After the operator is started, Percona Distribution for PostgreSQL     can be created at any time with the following command:</p> <pre><code>$ oc apply -f deploy/cr.yaml\n</code></pre> <p>Creation process will take some time. The process is over when both operator and replica set pod have reached their Running status:</p> <pre><code>$ oc get pods\n</code></pre> Expected output <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\nbackrest-backup-cluster1-j275w                    0/1     Completed 0          10m\ncluster1-85486d645f-gpxzb                         1/1     Running   0          10m\ncluster1-backrest-shared-repo-6495464548-c8wvl    1/1     Running   0          10m\ncluster1-pgbouncer-fc45869f7-s86rf                1/1     Running   0          10m\npgo-deploy-rhv6k                                  0/1     Completed 0          5m\npostgres-operator-8646c68b57-z8m62                4/4     Running   1          5m\n</code></pre> </li> <li> <p>During previous steps, the Operator has generated several secrets, including the password for the <code>pguser</code> user, which you will need to access the cluster.</p> <p>Use <code>oc get secrets</code> command to see the list of Secrets objects (by default Secrets object you are interested in has <code>cluster1-pguser-secret</code> name). Then you can use <code>oc get secret cluster1-pguser-secret -o yaml</code> to look through the YAML file with generated secrets (the actual password will be base64-encoded), or just get the needed password with the following command:</p> <pre><code>$ oc get secrets cluster1-users -o yaml -o jsonpath='{.data.pguser}' | base64 --decode | tr '\\n' ' ' &amp;&amp; echo \" \"\n</code></pre> <p>Here the actual password is base64-encoded, and <code>echo 'cGd1c2VyX3Bhc3N3b3JkCg==' | base64 --decode</code> will bring it back to a human-readable form (in this example it will be a <code>pguser_password</code> string).</p> </li> <li> <p>Check connectivity to newly created cluster. Run a new Pod to use it as a client and connect its console output to your terminal (running it may require some time to deploy). When you see the command line prompt of the newly created Pod, run <code>psql</code> tool using the password obtained from the secret. The following command will do this, naming the new Pod <code>pg-client</code>:</p> <pre><code>$ oc run -i --rm --tty pg-client --image=perconalab/percona-distribution-postgresql:{{ postgresrecommended }} --restart=Never -- bash -il\n[postgres@pg-client /]$ PGPASSWORD='pguser_password' psql -h cluster1-pgbouncer -p 5432 -U pguser pgdb\n</code></pre> <p>This command will connect you to the PostgreSQL interactive terminal.</p> <pre><code>$ psql ({{ postgresrecommended }})\nType \"help\" for help.\npgdb=&gt;\n</code></pre> </li> </ol>"},{"location":"openshift.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"operator.html","title":"Custom Resource options","text":"<p>The Cluster is configured via the deploy/cr.yaml file.</p> <p>The metadata part of this file contains the following keys:</p> <ul> <li><code>name</code> (<code>cluster1</code> by default) sets the name of your Percona Distribution for PostgreSQL Cluster; it should include only URL-compatible characters, not exceed 22 characters, start with an alphabetic character, and end with an alphanumeric character;</li> </ul> <p>The spec part of the deploy/cr.yaml file contains the following sections:</p> Key Value type Default Description pause boolean <code>false</code> Pause/resume: setting it to <code>true</code> gracefully stops the cluster, and setting it to <code>false</code> after shut down starts the cluster back. upgradeOptions subdoc Percona Distribution for PostgreSQL upgrade options section pgPrimary subdoc PostgreSQL Primary instance options section walStorage subdoc Tablespaces Storage Section walStorage subdoc Write-ahead Log Storage Section backup subdoc Section to configure backups and pgBackRest pmm subdoc Percona Monitoring and Management section pgBouncer subdoc The pgBouncer connection pooler section pgReplicas subdoc Section required to manage the replicas within a PostgreSQL cluster pgBadger subdoc The pgBadger PostgreSQL log analyzer section Key {{ optionlink(\u2018database\u2019,\u2019spec\u2019) }} Value string Example <code>pgdb</code> Description The name of a database that the PostgreSQL user can log into after the PostgreSQL cluster is created Key {{ optionlink(\u2018disableAutofail\u2019,\u2019spec\u2019) }} Value boolean Example <code>false</code> Description Turns high availability on or off. By default, every cluster can have high availability if there is at least one replica Key {{ optionlink(\u2018tlsOnly\u2019,\u2019spec\u2019) }} Value boolean Example <code>false</code> Description Enforece Operator to use only Transport Layer Security (TLS) for both internal and external communications Key {{ optionlink(\u2018sslCA\u2019,\u2019spec\u2019) }} Value string Example <code>cluster1-ssl-ca</code> Description The name of the secret with TLS  used for both connection encryption (external traffic), and replication (internal traffic) Key {{ optionlink(\u2018secretsName\u2019,\u2019spec\u2019) }} Value string Example <code>cluster1-secrets</code> Description The name of the secret created to store credentials for system users Key {{ optionlink(\u2018sslSecretName\u2019,\u2019spec\u2019) }} Value string Example <code>cluster1-ssl-keypair</code> Description The name of the secret created to encrypt external communications Key {{ optionlink(\u2018sslReplicationSecretName\u2019,\u2019spec\u2019) }} Value string Example <code>cluster1-ssl-keypair\"</code> Description The name of the secret created to encrypt internal communications Key {{ optionlink(\u2018keepData\u2019,\u2019spec\u2019) }} Value boolean Example <code>true</code> Description If <code>true</code>, PVCs will be kept after the cluster deletion Key {{ optionlink(\u2018keepBackups\u2019,\u2019spec\u2019) }} Value boolean Example <code>true</code> Description If <code>true</code>, local backups will be kept after the cluster deletion Key {{ optionlink(\u2018pgDataSource.restoreFrom\u2019) }} Value string Example <code>\"\"</code> Description The name of a data source PostgreSQL cluster, which is used to restore backup to a new cluster Key {{ optionlink(\u2018pgDataSource.restoreOpts\u2019) }} Value string Example <code>\"\"</code> Description Custom pgBackRest options to restore backup to a new cluster"},{"location":"operator.html#upgrade-options-section","title":"Upgrade Options Section","text":"<p>The <code>upgradeOptions</code> section in the deploy/cr.yaml file contains various configuration options to control Percona Distribution for PostgreSQL upgrades.</p> Key {{ optionlink(\u2018upgradeOptions.versionServiceEndpoint\u2019) }} Value string Example <code>https://check.percona.com</code> Description The Version Service URL used to check versions compatibility for upgrade Key {{ optionlink(\u2018upgradeOptions.apply\u2019) }} Value string Example <code>disabled</code> Description Specifies how updates are processed by the Operator. <code>Never</code> or <code>Disabled</code> will completely disable automatic upgrades, otherwise it can be set to <code>Latest</code> or <code>Recommended</code> or to a specific version number of Percona Distribution for PostgreSQL to have it version-locked (so that the user can control the version running, but use automatic upgrades to move between them). Key {{ optionlink(\u2018upgradeOptions.schedule\u2019) }} Value string Example <code>0 2 \\* \\* \\*</code> Description Scheduled time to check for updates, specified in the crontab format"},{"location":"operator.html#pgprimary-section","title":"pgPrimary Section","text":"<p>The pgPrimary section controls the PostgreSQL Primary instance.</p> Key {{ optionlink(\u2018pgPrimary.image\u2019) }} Value string Example <code>perconalab/percona-postgresql-operator:main-ppg13-postgres-ha</code> Description The Docker image of the PostgreSQL Primary instance Key {{ optionlink(\u2018pgPrimary.imagePullPolicy\u2019) }} Value string Example <code>Always</code> Description This option is used to set the policy for updating pgPrimary and pgReplicas images Key {{ optionlink(\u2018pgPrimary.resources.requests.memory\u2019) }} Value int Example <code>256Mi</code> Description The Kubernetes memory requests for a PostgreSQL Primary container Key {{ optionlink(\u2018pgPrimary.resources.requests.cpu\u2019) }} Value string Example <code>500m</code> Description Kubernetes CPU requests for a PostgreSQL Primary container Key {{ optionlink(\u2018pgPrimary.resources.limits.cpu\u2019) }} Value string Example <code>500m</code> Description Kubernetes CPU limits for a PostgreSQL Primary container Key {{ optionlink(\u2018pgPrimary.resources.limits.memory\u2019) }} Value string Example <code>256Mi</code> Description The Kubernetes memory limits for a PostgreSQL Primary container Key {{ optionlink(\u2018pgPrimary.affinity.antiAffinityType\u2019) }} Value string Example <code>preferred</code> Description Pod anti-affinity type, can be either <code>preferred</code> or <code>required</code> Key {{ optionlink(\u2018pgPrimary.affinity.nodeAffinityType\u2019) }} Value string Example <code>preferred</code> Description Node affinity type, can be either <code>preferred</code> or <code>required</code> Key {{ optionlink(\u2018pgPrimary.affinity.nodeLabel\u2019) }} Value label Example <code>kubernetes.io/region: us-central1</code> Description Set labels for PostgreSQL instances Node affinity Key {{ optionlink(\u2018pgPrimary.affinity.advanced\u2019) }} Value subdoc Example Description Allows using standard Kubernetes affinity constraints for advanced affinity and anti-affinity tuning Key {{ optionlink(\u2018pgPrimary.volumeSpec.size\u2019) }} Value int Example <code>1G</code> Description The Kubernetes PersistentVolumeClaim size for the PostgreSQL Primary storage Key {{ optionlink(\u2018pgPrimary.tolerations\u2019) }} Value subdoc Example <code>node.alpha.kubernetes.io/unreachable</code> Description Kubernetes Pod tolerations Key {{ optionlink(\u2018pgPrimary.volumeSpec.size\u2019) }} Value int Example <code>1G</code> Description The Kubernetes PersistentVolumeClaim size for the PostgreSQL Primary storage Key {{ optionlink(\u2018pgPrimary.volumeSpec.accessmode\u2019) }} Value string Example <code>ReadWriteOnce</code> Description The Kubernetes PersistentVolumeClaim access modes for the PostgreSQL Primary storage Key {{ optionlink(\u2018pgPrimary.volumeSpec.storagetype\u2019) }} Value string Example <code>dynamic</code> Description Type of the PostgreSQL Primary storage provisioning: <code>create</code> (the default variant; used if storage is provisioned, e.g. using hostpath) or <code>dynamic</code> (for a dynamic storage provisioner, e.g. via a StorageClass) Key {{ optionlink(\u2018pgPrimary.volumeSpec.storageclass\u2019) }} Value string Example <code>\"\"</code> Description Optionally sets the Kubernetes storage class to use with the PostgreSQL Primary storage PersistentVolumeClaim Key {{ optionlink(\u2018pgPrimary.volumeSpec.matchLabels\u2019) }} Value string Example <code>\"\"</code> Description A PostgreSQL Primary storage label selector Key {{ optionlink(\u2018pgPrimary.imagePullPolicy\u2019) }} Value string Example <code>Always</code> Description This option is used to set the policy for updating pgPrimary and pgReplicas images Key {{ optionlink(\u2018pgPrimary.expose.serviceType\u2019) }} Value string Example <code>ClusterIP</code> Description Specifies the type of Kubernetes Service for pgPrimary Key {{ optionlink(\u2018pgPrimary.expose.loadBalancerIP\u2019) }} Value string Example <code>127.0.0.1</code> Description The static IP-address for the load balancer Key {{ optionlink(\u2018pgPrimary.expose.loadBalancerSourceRanges\u2019) }} Value string Example <code>\"10.0.0.0/8\"</code> Description The range of client IP addresses from which the load balancer should be reachable (if not set, there is no limitations) Key {{ optionlink(\u2018pgPrimary.expose.annotations\u2019) }} Value label Example <code>pg-cluster-annot: cluster1</code> Description The Kubernetes annotations metadata for pgPrimary Key {{ optionlink(\u2018pgPrimary.expose.labels\u2019) }} Value label Example <code>pg-cluster-label: cluster1</code> Description Set labels for the pgPrimary Service Key {{ optionlink(\u2018pgPrimary.customconfig\u2019) }} Value string Example <code>\"\"</code> Description Name of the Custom configuration options ConfigMap for PostgreSQL cluster"},{"location":"operator.html#tablespaces-storage-section","title":"Tablespaces Storage Section","text":"<p>The <code>tablespaceStorages</code> section in the deploy/cr.yaml file contains configuration options for PostgreSQL Tablespace.</p> Key {{ optionlink(\u2018tablespaceStorages.&lt;storage-name&gt;.volumeSpec.size\u2019) }} Value int Example <code>1G</code> Description The Kubernetes PersistentVolumeClaim size for the PostgreSQL Tablespaces storage Key {{ optionlink(\u2018tablespaceStorages.&lt;storage-name&gt;.volumeSpec.accessmode\u2019) }} Value string Example <code>ReadWriteOnce</code> Description The Kubernetes PersistentVolumeClaim access modes for the PostgreSQL Tablespaces storage Key {{ optionlink(\u2018tablespaceStorages.&lt;storage-name&gt;.volumeSpec.storagetype\u2019) }} Value string Example <code>dynamic</code> Description Type of the PostgreSQL Tablespaces storage provisioning: <code>create</code> (the default variant; used if storage is provisioned, e.g. using hostpath) or <code>dynamic</code> (for a dynamic storage provisioner, e.g. via a StorageClass) Key {{ optionlink(\u2018tablespaceStorages.&lt;storage-name&gt;.volumeSpec.storageclass\u2019) }} Value string Example <code>\"\"</code> Description Optionally sets the Kubernetes storage class to use with the PostgreSQL Tablespaces storage PersistentVolumeClaim Key {{ optionlink(\u2018tablespaceStorages.&lt;storage-name&gt;.volumeSpec.matchLabels\u2019) }} Value string Example <code>\"\"</code> Description A PostgreSQL Tablespaces storage label selector"},{"location":"operator.html#write-ahead-log-storage-section","title":"Write-ahead Log Storage Section","text":"<p>The <code>walStorage</code> section in the deploy/cr.yaml file contains configuration options for PostgreSQL write-ahead logging.</p> Key {{ optionlink(\u2018walStorage.volumeSpec.size\u2019) }} Value int Example <code>1G</code> Description The Kubernetes PersistentVolumeClaim size for the PostgreSQL Write-ahead Log storage Key {{ optionlink(\u2018walStorage.volumeSpec.accessmode\u2019) }} Value string Example <code>ReadWriteOnce</code> Description The Kubernetes PersistentVolumeClaim access modes for the PostgreSQL Write-ahead Log storage Key {{ optionlink(\u2018walStorage.volumeSpec.storagetype\u2019) }} Value string Example <code>dynamic</code> Description Type of the PostgreSQL Write-ahead Log storage provisioning: <code>create</code> (the default variant; used if storage is provisioned, e.g. using hostpath) or <code>dynamic</code> (for a dynamic storage provisioner, e.g. via a StorageClass) Key {{ optionlink(\u2018walStorage.volumeSpec.storageclass\u2019) }} Value string Example <code>\"\"</code> Description Optionally sets the Kubernetes storage class to use with the PostgreSQL Write-ahead Log storage PersistentVolumeClaim Key {{ optionlink(\u2018walStorage.volumeSpec.matchLabels\u2019) }} Value string Example <code>\"\"</code> Description A PostgreSQL Write-ahead Log storage label selector"},{"location":"operator.html#backup-section","title":"Backup Section","text":"<p>The <code>backup</code> section in the deploy/cr.yaml file contains the following configuration options for the regular Percona Distribution for PostgreSQL backups.</p> Key {{ optionlink(\u2018backup.image\u2019) }} Value string Example <code>perconalab/percona-postgresql-operator:main-ppg13-pgbackrest</code> Description The Docker image for pgBackRest Key {{ optionlink(\u2018backup.backrestRepoImage\u2019) }} Value string Example <code>perconalab/percona-postgresql-operator:main-ppg13-pgbackrest-repo</code> Description The Docker image for the BackRest repository Key {{ optionlink(\u2018backup.resources.requests.cpu\u2019) }} Value string Example <code>500m</code> Description Kubernetes CPU requests for a pgBackRest container Key {{ optionlink(\u2018backup.resources.requests.memory\u2019) }} Value int Example <code>48Mi</code> Description The Kubernetes memory requests for a pgBackRest container Key {{ optionlink(\u2018backup.resources.limits.cpu\u2019) }} Value int Example <code>1</code> Description Kubernetes CPU limits for a pgBackRest container Key {{ optionlink(\u2018backup.resources.limits.memory\u2019) }} Value int Example <code>64Mi</code> Description The Kubernetes memory limits for a pgBackRest container Key {{ optionlink(\u2018backup.affinity.antiAffinityType\u2019) }} Value string Example <code>preferred</code> Description Pod anti-affinity type, can be either <code>preferred</code> or <code>required</code> Key {{ optionlink(\u2018backup.volumeSpec.size\u2019) }} Value int Example <code>1G</code> Description The Kubernetes PersistentVolumeClaim size for the pgBackRest Storage Key {{ optionlink(\u2018backup.volumeSpec.accessmode\u2019) }} Value string Example <code>ReadWriteOnce</code> Description The Kubernetes PersistentVolumeClaim access modes for the pgBackRest Storage Key {{ optionlink(\u2018backup.volumeSpec.storagetype\u2019) }} Value string Example <code>dynamic</code> Description Type of the pgBackRest storage provisioning: <code>create</code> (the default variant; used if storage is provisioned, e.g. using hostpath) or <code>dynamic</code> (for a dynamic storage provisioner, e.g. via a StorageClass) Key {{ optionlink(\u2018backup.volumeSpec.storageclass\u2019) }} Value string Example <code>\"\"</code> Description Optionally sets the Kubernetes storage class to use with the pgBackRest Storage PersistentVolumeClaim Key {{ optionlink(\u2018backup.volumeSpec.matchLabels\u2019) }} Value string Example <code>\"\"</code> Description A pgBackRest storage label selector Key {{ optionlink(\u2018backup.storages.&lt;storage-name&gt;.type\u2019) }} Value string Example <code>s3</code> Description Type of the storage used for backups Key {{ optionlink(\u2018backup.storages.&lt;storage-name&gt;.endpointURL\u2019) }} Value string Example <code>minio-gateway-svc:9000</code> Description The endpoint URL of the S3-compatible storage to be used for backups (not needed for the original Amazon S3 cloud) Key {{ optionlink(\u2018backup.storages.&lt;storage-name&gt;.bucket\u2019) }} Value string Example <code>\"\"</code> Description The Amazon S3 bucket or Google Cloud Storage bucket name used for backups Key {{ optionlink(\u2018backup.storages.&lt;storage-name&gt;.region\u2019) }} Value boolean Example <code>us-east-1</code> Description The AWS region to use for Amazon and all S3-compatible storages Key {{ optionlink(\u2018backup.storages.&lt;storage-name&gt;.uriStyle\u2019) }} Value string Example <code>path</code> Description Optional parameter that specifies if pgBackRest should use the path or host S3 URI style Key {{ optionlink(\u2018backup.storages.&lt;storage-name&gt;.verifyTLS\u2019) }} Value boolean Example <code>false</code> Description Enables or disables TLS verification for pgBackRest Key {{ optionlink(\u2018backup.storageTypes\u2019) }} Value array Example <code>[ \"s3\" ]</code> Description The backup storage types for the pgBackRest repository Key {{ optionlink(\u2018backup.repoPath\u2019) }} Value string Example <code>\"\"</code> Description Custom path for pgBackRest repository backups Key {{ optionlink(\u2018backup.schedule.name\u2019) }} Value string Example <code>sat-night-backup</code> Description The backup name Key {{ optionlink(\u2018backup.schedule.schedule\u2019) }} Value string Example <code>0 0 \\* \\* 6</code> Description Scheduled time to make a backup specified in the crontab format Key {{ optionlink(\u2018backup.schedule.keep\u2019) }} Value int Example <code>3</code> Description The amount of most recent backups to store. Older backups are automatically deleted. Set <code>keep</code> to zero or completely remove it to disable automatic deletion of backups Key {{ optionlink(\u2018backup.schedule.type\u2019) }} Value string Example <code>full</code> Description The type of the pgBackRest backup Key {{ optionlink(\u2018backup.schedule.storage\u2019) }} Value string Example <code>local</code> Description The type of the pgBackRest repository Key {{ optionlink(\u2018backup.schedule.backrestOpts\u2019) }} Value string Example <code>--annotation=source=scheduled-backup</code> Description Custom pgBackRest configuration options for scheduled backups Key {{ optionlink(\u2018backup.customconfig\u2019) }} Value string Example <code>\"\"</code> Description Name of the ConfigMap to pass custom pgBackRest configuration options Key {{ optionlink(\u2018backup.imagePullPolicy\u2019) }} Value string Example <code>Always</code> Description This option is used to set the policy for updating pgBackRest images"},{"location":"operator.html#pmm-section","title":"PMM Section","text":"<p>The <code>pmm</code> section in the deploy/cr.yaml file contains configuration options for Percona Monitoring and Management.</p> Key {{ optionlink(\u2018pmm.enabled\u2019) }} Value boolean Example <code>false</code> Description Enables or disables monitoring Percona Distribution for PostgreSQL cluster with PMM Key {{ optionlink(\u2018pmm.image\u2019) }} Value string Example <code>percona/pmm-client:{{ pmm2recommended }}</code> Description Percona Monitoring and Management (PMM) Client Docker image Key {{ optionlink(\u2018pmm.serverHost\u2019) }} Value string Example <code>monitoring-service</code> Description Address of the PMM Server to collect data from the cluster Key {{ optionlink(\u2018pmm.serverUser\u2019) }} Value string Example <code>admin</code> Description The PMM Server User. The PMM Server password should be configured using Secrets Key {{ optionlink(\u2018pmm.pmmSecret\u2019) }} Value string Example <code>cluster1-pmm-secret</code> Description Name of the Kubernetes Secret object for the PMM Server password Key {{ optionlink(\u2018pmm.resources.requests.memory\u2019) }} Value string Example <code>200M</code> Description The Kubernetes memory requests for a PMM container Key {{ optionlink(\u2018pmm.resources.requests.cpu\u2019) }} Value string Example <code>500m</code> Description Kubernetes CPU requests for a PMM container Key {{ optionlink(\u2018pmm.resources.limits.cpu\u2019) }} Value string Example <code>500m</code> Description Kubernetes CPU limits for a PMM container Key {{ optionlink(\u2018pmm.resources.limits.memory\u2019) }} Value string Example <code>200M</code> Description The Kubernetes memory limits for a PMM container Key {{ optionlink(\u2018pmm.imagePullPolicy\u2019) }} Value string Example <code>Always</code> Description This option is used to set the policy for updating PMM Client images"},{"location":"operator.html#pgbouncer-section","title":"pgBouncer Section","text":"<p>The <code>pgBouncer</code> section in the deploy/cr.yaml file contains configuration options for the pgBouncer connection pooler for PostgreSQL.</p> Key {{ optionlink(\u2018pgBouncer.image\u2019) }} Value string Example <code>perconalab/percona-postgresql-operator:main-ppg13-pgbouncer</code> Description Docker image for the pgBouncer connection pooler Key {{ optionlink(\u2018pgBouncer.exposePostgresUser\u2019) }} Value boolean Example <code>false</code> Description Enables or disables exposing postgres user through pgBouncer Key {{ optionlink(\u2018pgBouncer.size\u2019) }} Value int Example <code>1G</code> Description The number of the pgBouncer Pods to provide connection pooling Key {{ optionlink(\u2018pgBouncer.resources.requests.cpu\u2019) }} Value int Example <code>1</code> Description Kubernetes CPU requests for a pgBouncer container Key {{ optionlink(\u2018pgBouncer.resources.requests.memory\u2019) }} Value int Example <code>128Mi</code> Description The Kubernetes memory requests for a pgBouncer container Key {{ optionlink(\u2018pgBouncer.resources.limits.cpu\u2019) }} Value int Example <code>2</code> Description Kubernetes CPU limits for a pgBouncer container Key {{ optionlink(\u2018pgBouncer.resources.limits.memory\u2019) }} Value int Example <code>512Mi</code> Description The Kubernetes memory limits for a pgBouncer container Key {{ optionlink(\u2018pgBouncer.affinity.antiAffinityType\u2019) }} Value string Example <code>preferred</code> Description Pod anti-affinity type, can be either <code>preferred</code> or <code>required</code> Key {{ optionlink(\u2018pgBouncer.expose.serviceType\u2019) }} Value string Example <code>ClusterIP</code> Description Specifies the type of Kubernetes Service for pgBouncer Key {{ optionlink(\u2018pgBouncer.expose.loadBalancerIP\u2019) }} Value string Example <code>127.0.0.1</code> Description The static IP-address for the load balancer Key {{ optionlink(\u2018pgBouncer.expose.loadBalancerSourceRanges\u2019) }} Value string Example <code>\"10.0.0.0/8\"</code> Description The range of client IP addresses from which the load balancer should be reachable (if not set, there is no limitations) Key {{ optionlink(\u2018pgBouncer.expose.annotations\u2019) }} Value label Example <code>pg-cluster-annot: cluster1</code> Description The Kubernetes annotations metadata for pgBouncer Key {{ optionlink(\u2018pgBouncer.expose.labels\u2019) }} Value label Example <code>pg-cluster-label: cluster1</code> Description Set labels for the pgBouncer Service Key {{ optionlink(\u2018pgBouncer.imagePullPolicy\u2019) }} Value string Example <code>Always</code> Description This option is used to set the policy for updating pgBouncer images"},{"location":"operator.html#pgreplicas-section","title":"pgReplicas Section","text":"<p>The <code>pgReplicas</code> section in the deploy/cr.yaml file stores information required to manage the replicas within a PostgreSQL cluster.</p> Key {{ optionlink(\u2018pgReplicas..size\u2019) }} Value int Example <code>1G</code> Description The number of the PostgreSQL Replica Pods Key {{ optionlink(\u2018pgReplicas..resources.requests.cpu\u2019) }} Value int Example <code>500m</code> Description Kubernetes CPU requests for a PostgreSQL Replica container Key {{ optionlink(\u2018pgReplicas..resources.requests.memory\u2019) }} Value int Example <code>256Mi</code> Description The Kubernetes memory requests for a PostgreSQL Replica container Key {{ optionlink(\u2018pgReplicas..resources.limits.cpu\u2019) }} Value int Example <code>500m</code> Description Kubernetes CPU limits for a PostgreSQL Replica container Key {{ optionlink(\u2018pgReplicas..resources.limits.memory\u2019) }} Value int Example <code>256Mi</code> Description The Kubernetes memory limits for a PostgreSQL Replica container Key {{ optionlink(\u2018pgReplicas..volumeSpec.accessmode\u2019) }} Value string Example <code>ReadWriteOnce</code> Description The Kubernetes PersistentVolumeClaim access modes for the PostgreSQL Replica storage Key {{ optionlink(\u2018pgReplicas..volumeSpec.size\u2019) }} Value int Example <code>1G</code> Description The Kubernetes PersistentVolumeClaim size for the PostgreSQL Replica storage Key {{ optionlink(\u2018pgReplicas..volumeSpec.storagetype\u2019) }} Value string Example <code>dynamic</code> Description Type of the PostgreSQL Replica storage provisioning: <code>create</code> (the default variant; used if storage is provisioned, e.g. using hostpath) or <code>dynamic</code> (for a dynamic storage provisioner, e.g. via a StorageClass) Key {{ optionlink(\u2018pgReplicas..volumeSpec.storageclass\u2019) }} Value string Example <code>standard</code> Description Optionally sets the Kubernetes storage class to use with the PostgreSQL Replica storage PersistentVolumeClaim Key {{ optionlink(\u2018pgReplicas..volumeSpec.matchLabels\u2019) }} Value string Example <code>\"\"</code> Description A PostgreSQL Replica storage label selector Key {{ optionlink(\u2018pgReplicas..labels\u2019) }} Value label Example <code>pg-cluster-label: cluster1</code> Description Set labels for PostgreSQL Replica Pods Key {{ optionlink(\u2018pgReplicas..annotations\u2019) }} Value label Example <code>pg-cluster-annot: cluster1-1</code> Description The Kubernetes annotations metadata for PostgreSQL Replica Key {{ optionlink(\u2018pgReplicas..expose.serviceType\u2019) }} Value string Example <code>ClusterIP</code> Description Specifies the type of Kubernetes Service for for PostgreSQL Replica Key {{ optionlink(\u2018pgReplicas..expose.loadBalancerSourceRanges\u2019) }} Value string Example <code>\"10.0.0.0/8\"</code> Description The range of client IP addresses from which the load balancer should be reachable (if not set, there is no limitations) Key {{ optionlink(\u2018pgReplicas..expose.annotations\u2019) }} Value label Example <code>pg-cluster-annot: cluster1</code> Description The Kubernetes annotations metadata for PostgreSQL Replica Key {{ optionlink(\u2018pgReplicas..expose.labels\u2019) }} Value label Example <code>pg-cluster-label: cluster1</code> Description Set labels for the PostgreSQL Replica Service"},{"location":"operator.html#pgbadger-section","title":"pgBadger Section","text":"<p>The <code>pgBadger</code> section in the deploy/cr.yaml file contains configuration options for the pgBadger PostgreSQL log analyzer.</p> Key {{ optionlink(\u2018pgBadger.enabled\u2019) }} Value boolean Example <code>false</code> Description Enables or disables the pgBadger PostgreSQL log analyzer Key {{ optionlink(\u2018pgBadger.image\u2019) }} Value string Example <code>perconalab/percona-postgresql-operator:main-ppg13-pgbadger</code> Description pgBadger PostgreSQL log analyzer Docker image Key {{ optionlink(\u2018pgBadger.port\u2019) }} Value int Example <code>10000</code> Description The port number for pgBadger Key {{ optionlink(\u2018pgBadger.imagePullPolicy\u2019) }} Value string Example <code>Always</code> Description This option is used to set the policy for updating pgBadger images"},{"location":"operator.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"options.html","title":"Changing PostgreSQL Options","text":"<p>You may require a configuration change for your application. PostgreSQL allows customizing the database with configuration files. You can use a ConfigMap to provide the PostgreSQL configuration options specific to the following configuration files:</p> <ul> <li>PostgreSQL main configuration, postgresql.conf,</li> <li>client authentication configuration, pg_hba.conf,</li> <li>user name configuration, pg_ident.conf.</li> </ul> <p>Configuration options may be applied in two ways:</p> <ul> <li>globally to all database servers in the cluster via Patroni Distributed Configuration Store (DCS),</li> <li>locally to each database server (Primary and Replica) within the cluster.</li> </ul> <p>Note</p> <p>PostgreSQL cluster is managed by the Operator, and so there is no need to set custom configuration options in common usage scenarios. Also, changing certain options may cause PostgreSQL cluster malfunction. Do not customize configuration unless you know what you are doing!</p> <p>Use the <code>kubectl</code> command to create the ConfigMap from external resources, for more information, see Configure a Pod to use a ConfigMap.</p> <p>You can either create a PostgreSQL Cluster With Custom Configuration, or use ConfigMap to set options for the already existing cluster.</p> <p>To create a cluster with custom options, you should first place these options in a <code>postgres-ha.yaml</code> file under specific <code>bootstrap</code> section, then use <code>kubectl create configmap</code> command with this file to create a ConfigMap, and finally put the ConfigMap name to pgPrimary.customconfig key in the <code>deploy/cr.yaml</code> configuration file.</p> <p>In both cases, the <code>postgres-ha.yaml</code> file doesn\u2019t fully overwrite PostgreSQL configuration files: options present in <code>postgres-ha.yaml</code> will be overwritten, while non-present options will be left intact.</p>"},{"location":"options.html#creating-a-cluster-with-custom-options","title":"Creating a cluster with custom options","text":"<p>For example, you can create a cluster with a custom <code>max_connections</code> option in a <code>postgresql.conf</code> configuration file using the following <code>postgres-ha.yaml</code> contents:</p> <pre><code>---\nbootstrap:\n  dcs:\n    postgresql:\n      parameters:\n        max_connections: 30\n</code></pre> <p>Note</p> <p><code>dsc.postgresql</code> subsection means that option will be applied globally to <code>postgresql.conf</code> of all database servers.</p> <p>You can create a ConfigMap from this file. The syntax for <code>kubectl create configmap</code> command is:</p> <pre><code>kubectl -n &lt;namespace&gt; create configmap &lt;configmap-name&gt; --from-file=postgres-ha.yaml\n</code></pre> <p>ConfigMap name should include your cluster name and a dash as a prefix (<code>cluster1-</code> by default).</p> <p>The following example defines <code>cluster1-custom-config</code> as the ConfigMap name:</p> <pre><code>$ kubectl create -n pgo configmap cluster1-custom-config --from-file=postgres-ha.yaml\n</code></pre> <p>To view the created ConfigMap, use the following command:</p> <pre><code>$ kubectl describe configmaps cluster1-custom-config\n</code></pre> <p>Don\u2019t forget to put the name of your ConfigMap to the <code>deploy/cr.yaml</code> configuration file:</p> <pre><code>spec:\n  ...\n  pgPrimary:\n    ...\n      customconfig: \"cluster1-custom-config\"\n</code></pre> <p>Now you can create the cluster following the regular installation instructions.</p>"},{"location":"options.html#modifying-options-for-the-existing-cluster","title":"Modifying options for the existing cluster","text":"<p>If you need to update cluster\u2019s configuration settings, you should modify settings in the <code>&lt;clusterName&gt;-pgha-config</code> ConfigMap.</p> <p>Note</p> <p>This ConfigMap contains <code>&lt;clusterName&gt;-dcs-config</code> configuration applied globally to <code>postgresql.conf</code> of all database servers, and local configurations for the PostgreSQL cluster database servers: <code>&lt;clusterName&gt;-local-config</code> for the current primary, <code>&lt;clusterName&gt;-repl1-local-config</code> for the first replica, and so on.</p> <p>For example, let\u2019s change the <code>max_connections</code> option in a globally applied <code>postgresql.conf</code> configuration file for the cluster named <code>cluster1</code>. Edit the <code>cluster1-pgha-config</code> ConfigMap with the following command:</p> <pre><code>$ kubectl edit -n pgo configmap cluster1-pgha-config\n</code></pre> <p>This will open the ConfigMap in a local text editor of your choice. Make sure to modify it as follows:</p> <pre><code>...\ncluster1-dcs-config: |\n  postgresql:\n    parameters:\n     ...\n     max_connections: 50\n     ...\n</code></pre> <p>Now restart the cluster to ensure the update took effect.</p> <p>You can check if the changes are applied by querying the appropriate Pods of your cluster using the <code>kubectl exec</code> command with a specific Pod name.</p> <p>First find out names of your Pods in a common way, using the <code>kubectl get pods</code> command:</p> <pre><code>$ kubectl get pods\n</code></pre> Expected output <pre><code>NAME                                              READY   STATUS    RESTARTS   AGE\nbackrest-backup-cluster1-j275w                    0/1     Completed 0          10m\ncluster1-85486d645f-gpxzb                         1/1     Running   0          10m\ncluster1-backrest-shared-repo-6495464548-c8wvl    1/1     Running   0          10m\ncluster1-pgbouncer-fc45869f7-s86rf                1/1     Running   0          10m\npgo-deploy-rhv6k                                  0/1     Completed 0          5m\npostgres-operator-8646c68b57-z8m62                4/4     Running   1          5m\n</code></pre> <p>Now let\u2019s check the <code>cluster1-85486d645f-gpxzb</code> Pod for the current <code>max_connections</code> value:</p> <pre><code>$ kubectl -n pgo exec -it cluster1-85486d645f-gpxzb -- psql -c 'show max_connections;'\n</code></pre> Expected output <pre><code>max_connections\n-----------------\n50\n(1 row)\n</code></pre>"},{"location":"options.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"pause.html","title":"Pause/resume PostgreSQL Cluster","text":"<p>There may be external situations when it is needed to pause your Cluster for a while and then start it back up (some works related to the maintenance of the enterprise infrastructure, etc.).</p> <p>The <code>deploy/cr.yaml</code> file contains a special <code>spec.pause</code> key for this. Setting it to <code>true</code> gracefully stops the cluster:</p> <pre><code>spec:\n  .......\n  pause: true\n</code></pre> <p>To start the cluster after it was paused just revert the <code>spec.pause</code> key to <code>false</code>.</p> <p>Note</p> <p>There is an option also to put the cluster into a standby (read-only) mode instead of completely shutting it down. This is done by a special <code>spec.standby</code> key, which should be set to <code>true</code> for read-only state or should be set to <code>false</code> for normal cluster operation:</p> <pre><code>spec:\n  .......\n  standby: false\n</code></pre>"},{"location":"pause.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"private.html","title":"Creating a private S3-compatible cloud for backups","text":"<p>As it is mentioned in backups, any cloud storage which implements the S3 API can be used for backups. The one way to setup and implement the S3 API storage on Kubernetes or OpenShift is Minio - the S3-compatible object storage server deployed via Docker on your own infrastructure.</p> <p>Setting up Minio to be used with Percona Operator for PostgreSQL backups involves the following steps:</p> <ol> <li> <p>Install Minio in your Kubernetes or OpenShift     environment and create the correspondent Kubernetes Service as     follows:</p> <pre><code>$ helm install \\\n  --name minio-service \\\n  --version 8.0.5 \\\n  --set accessKey=some-access-key \\\n  --set secretKey=some-secret-key \\\n  --set service.type=ClusterIP \\\n  --set configPath=/tmp/.minio/ \\\n  --set persistence.size=2G \\\n  --set environment.MINIO_REGION=us-east-1 \\\n  stable/minio\n</code></pre> <p>Don\u2019t forget to substitute default <code>some-access-key</code> and <code>some-secret-key</code> strings in this command with actual unique key values. The values can be used later for access control. The <code>storageClass</code> option is needed if you are using the special Kubernetes Storage Class for backups. Otherwise, this setting may be omitted. You may also notice the <code>MINIO_REGION</code> value which is may not be used within a private cloud. Use the same region value here and on later steps (<code>us-east-1</code> is a good default choice).</p> </li> <li> <p>Create an S3 bucket for backups:</p> <pre><code>$ kubectl run -i --rm aws-cli --image=perconalab/awscli --restart=Never -- \\\n  bash -c 'AWS_ACCESS_KEY_ID=some-access-key \\\n  AWS_SECRET_ACCESS_KEY=some-secret-key \\\n  AWS_DEFAULT_REGION=us-east-1 \\\n  /usr/bin/aws \\\n  --endpoint-url http://minio-service:9000 \\\n  s3 mb s3://operator-testing'\n</code></pre> <p>This command creates the bucket named <code>operator-testing</code> with the selected access and secret keys (substitute <code>some-access-key</code> and <code>some-secret-key</code> with the values used on the previous step).</p> </li> <li> <p>Now edit the backup section of the deploy/cr.yaml     file to set proper values for your newly created  storage as follows (you     can find more on these options in backup and restore documentation).</p> <pre><code>...\nbackup:\n  ...\n  storages:\n    minio:\n      type: s3\n      bucket: operator-testing\n      region: us-east-1\n      endpointUrl: http://minio-service:9000\n      uriStyle: \"path\"\n      verifyTLS: false\n</code></pre> <p>You will also need to supply pgBackRest with base64-encoded access and  secret keys stored in Kubernetes Secrets.</p> <p>Note</p> <p>You can encode needed data to base64 with the following command:</p> in Linuxin macOS <pre><code>$ echo -n 'plain-text-string' | base64 --wrap=0\n</code></pre> <pre><code>$ echo -n 'plain-text-string' | base64\n</code></pre> <p>Edit the <code>deploy/backup/cluster1-backrest-repo-config-secret.yaml</code> configuration file: set <code>name</code>, <code>aws-s3-key</code>, and <code>aws-s3-key-secret</code> with proper cluster name, key, and key secret.</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: &lt;cluster-name&gt;-backrest-repo-config\ntype: Opaque\ndata:\n  aws-s3-key: c29tZS1hY2Nlc3Mta2V5\n  aws-s3-key-secret: c29tZS1zZWNyZXQta2V5\n</code></pre> <p>When done, create the secret as follows:</p> <pre><code>$ kubectl apply -f deploy/backup/cluster1-backrest-repo-config-secret.yaml\n</code></pre> <p>Finally, create or update the cluster:</p> <pre><code>$ kubectl apply -f deploy/cr.yaml\n</code></pre> </li> <li> <p>When the setup process is completed, you can make on-demand     and scheduled backups and/or backup restore following the     official backup/restore documentation.</p> </li> </ol>"},{"location":"private.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"scaling.html","title":"Scale Percona Distribution for PostgreSQL on Kubernetes and OpenShift","text":"<p>One of the great advantages brought by Kubernetes and the OpenShift platform is the ease of an application scaling. Scaling an application results in adding or removing the Pods and scheduling them to available Kubernetes nodes.</p> <p>Size of the cluster is dynamically controlled by a pgReplicas.REPLICA-NAME.size key in the Custom Resource options configuration.  That\u2019s why scaling the cluster needs nothing more but changing this option and applying the updated configuration file. This may be done in a specifically saved config, or on the fly, using the following command:</p> <pre><code>$ kubectl scale --replicas=5 perconapgcluster/cluster1\n</code></pre> <p>In this example we have changed the number of PostgreSQL Replicas to <code>5</code> instances.</p>"},{"location":"scaling.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"standby.html","title":"How to deploy a standby cluster for Disaster Recovery","text":"<p>Deployment of a standby PostgreSQL cluster is mainly targeted for Disaster Recovery (DR), though it can also be used for migrations.</p> <p>In both cases, it involves using some object storage system for backups, such as AWS S3 or GCP Cloud Storage, which the standby cluster can access:</p> <p></p> <ul> <li>there is a primary cluster with configured <code>pgbackrest</code> tool, which pushes the write-ahead log (WAL) archives to the correct remote repository,</li> <li>the standby cluster is built from one of these backups, and it is kept in sync with the primary cluster by consuming the WAL files copied from the remote repository.</li> </ul> <p>Note</p> <p>The primary node in the standby cluster is not a streaming replica from any of the nodes in the primary cluster. It relies only on WAL archives to replicate events. For this reason, this approach cannot be used as a High Availability solution.</p> <p>Creating such a standby cluster involves the following steps:</p> <ul> <li> <p>Copy needed passwords from the primary cluster Secrets and adjust them to use the standby cluster name. </p> <p>Note</p> <p>You need the yq tool installed.</p> <p>The following commands save the secrets files from <code>cluster1</code> under <code>/tmp/copied-secrets</code> directory and prepare them to be used in <code>cluster2</code>:</p> <pre><code>$ mkdir -p /tmp/copied-secrets/\n$ export primary_cluster_name=cluster1\n$ export standby_cluster_name=cluster2\n$ export secrets=\"${primary_cluster_name}-users\"\n$ kubectl get secret/$secrets -o yaml \\\nyq eval 'del(.metadata.creationTimestamp)' - \\\nyq eval 'del(.metadata.uid)' - \\\nyq eval 'del(.metadata.selfLink)' - \\\nyq eval 'del(.metadata.resourceVersion)' - \\\nyq eval 'del(.metadata.namespace)' - \\\nyq eval 'del(.metadata.annotations.\"kubectl.kubernetes.io/last-applied-configuration\")' - \\\nyq eval '.metadata.name = \"'\"${secrets/$primary_cluster_name/$standby_cluster_name}\"'\"' - \\\nyq eval '.metadata.labels.pg-cluster = \"'\"${standby_cluster_name}\"'\"' - \\\n&gt;/tmp/copied-secrets/${secrets/$primary_cluster_name/$standby_cluster_name}\n</code></pre> </li> <li> <p>Create the Operator in the Kubernetes environment for the standby cluster,     if not done:</p> <pre><code>$ kubectl apply -f deploy/operator.yaml\n</code></pre> </li> <li> <p>Apply the Adjusted Kubernetes Secrets:</p> <pre><code>$ export standby_cluster_name=cluster2\n$ kubectl create -f /tmp/copied-secrets/${standby_cluster_name}-users\n</code></pre> </li> <li> <p>Set the backup.repoPath option in the     <code>deploy/cr.yaml</code> file of your standby cluster to the actual place where     the primary cluster stores backups. If this option is not set in     <code>deploy/cr.yaml</code> of your primary cluster, then the following default     naming is used: <code>/backrestrepo/&lt;primary-cluster-name&gt;-backrest-shared-repo</code>.     For example, in case of <code>myPrimaryCluster</code> and <code>myStandbyCluster</code>     clusters, it should look as follows:</p> <pre><code>...\n  name: myStandbyCluster\n...\n  backup:\n    ...\n    repoPath: \"/backrestrepo/myPrimaryCluster-backrest-shared-repo\"\n</code></pre> </li> <li> <p>Supply your standby cluster with the Kubernetes Secret used by pgBackRest of     the primary cluster to Access the Storage Bucket. The name of this Secret is     <code>&lt;cluster-name&gt;-backrest-repo-config</code>, and its content depends on the cloud     used for backups (refer to the Operator\u2019s backups documentation     for this step). The contents of the Secret needs to be the same for both     primary and standby clusters except for the name: e.g.     <code>cluster1-backrest-repo-config</code> should be recreated as     <code>cluster2-backrest-repo-config</code>.</p> </li> <li> <p>Enable the standby option in your standby cluster\u2019s <code>deploy/cr.yaml</code> file:</p> <pre><code>standby: true\n</code></pre> </li> </ul> <p>When you have applied your new cluster configuration with the usual <code>kubectl -f deploy/cr.yaml</code> command, it starts the synchronization via pgBackRest, and your Disaster Recovery preparations are over.</p> <p>When you need to actually use your new cluster, get it out from standby mode, changing the standby option in your <code>deploy/cr.yaml</code> file:</p> <pre><code>standby: false\n</code></pre> <p>Please take into account, that your <code>cluster1</code> cluster should not exist at the moment when you get out your <code>cluster2</code> from standby:</p> <p></p> <p>Note</p> <p>If <code>cluster1</code> still exists for some reason, make sure it can not connect to backup storage. Otherwise, both clusters sending WAL archives to it would cause data corruption!</p>"},{"location":"standby.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"tablespace.html","title":"Using PostgreSQL tablespaces with Percona Operator for PostgreSQL","text":"<p>Tablespaces allow DBAs to store a database on multiple file systems within the same server and to control where (on which file systems) specific parts of the database are stored. You can think about it as if you were giving names to your disk mounts and then using those names as additional parameters when creating database objects.</p> <p>PostgreSQL supports this feature, allowing you to store data outside of the primary data directory, and Percona Operator for PostgreSQL is a good option to bring this to your Kubernetes environment when needed.</p>"},{"location":"tablespace.html#possible-use-cases","title":"Possible use cases","text":"<p>The most obvious use case for tablespaces is performance optimization. You place appropriate parts of the database on fast but expensive storage and engage slower but cheaper storage for lesser-used database objects. The classic example would be using an SSD for heavily-used indexes and using a large slow HDD for archive data.</p> <p>Of course, the Operator already provides you with traditional Kubernetes approaches to achieve this on a per-Pod basis (Tolerations, etc.). But if you would like to go deeper and make such differentiation at the level of your database objects (tables and indexes), tablespaces are exactly what you would need for that.</p> <p>Another well-known use case for tablespaces is quickly adding a new partition to the database cluster when you run out of space on the initially used one and cannot extend it (which may look less typical for cloud storage). Finally, you may need tablespaces when migrating your existing architecture to the cloud.</p> <p>Each tablespace created by Percona Operator for PostgreSQL corresponds to a separate Persistent Volume, mounted in a container to the <code>/tablespaces</code> directory.</p> <p></p>"},{"location":"tablespace.html#creating-a-new-tablespace","title":"Creating a new tablespace","text":"<p>Providing a new tablespace for your database in Kubernetes involves two parts:</p> <ol> <li>Configure the new tablespace storage with the Operator,</li> <li>Create database objects in this tablespace with PostgreSQL.</li> </ol> <p>The first part is done in the traditional way of Percona Operators, by modifying Custom Resource via the <code>deploy/cr.yaml</code> configuration file. It has a special spec.tablespaceStorages section with subsections names equal to PostgreSQL tablespace names.</p> <p>The example already present in <code>deploy/cr.yaml</code> shows how to create tablespace storage named <code>lake</code> 1Gb in size with dynamic provisioning (you can see official Kubernetes documentation on Persistent Volumes for details):</p> <pre><code>spec:\n...\n  tablespaceStorages:\n    lake:\n      volumeSpec:\n        size: 1G\n        accessmode: ReadWriteOnce\n        storagetype: dynamic\n        storageclass: \"\"\n        matchLabels: \"\"\n</code></pre> <p>After you apply this by running the <code>kubectl apply -f deploy/cr.yaml</code> command, the new <code>lake</code> tablespace will appear within your database. Please take into account that if you add your new tablespace to the already existing PostgreSQL cluster, it may take time for the Operator to create Persistent Volume Claims and get Persistent Volumes actually mounted.</p> <p>Now you can append <code>TABLESPACE &lt;tablespace_name&gt;</code> to your <code>CREATE</code> SQL statements to implicitly create tables, indexes, or even entire databases in specific tablespaces (of course, your user should have appropriate <code>CREATE</code> privileges to make it possible).</p> <p>Let\u2019s create an example table in the already mentioned <code>lake</code> tablespace:</p> <pre><code>CREATE TABLE products (\n    product_sku character(10),\n    quantity int,\n    manufactured_date timestamptz)\nTABLESPACE lake;\n</code></pre> <p>It is also possible to set a default tablespace with the <code>SET default_tablespace = &lt;tablespace_name&gt;;</code> statement. It will affect all further <code>CREATE TABLE</code> and <code>CREATE INDEX</code> commands without an explicit tablespace specifier, until you unset it with an empty string.</p> <p>As you can see, Percona Operator for PostgreSQL simplifies tablespace creation by carrying on all necessary modifications with Persistent Volumes and Pods. The same would not be true for the deletion of an already existing tablespace, which is not automated, neither by the Operator nor by PostgreSQL.</p>"},{"location":"tablespace.html#deleting-an-existing-tablespace","title":"Deleting an existing tablespace","text":"<p>Deleting an existing tablespace from your database in Kubernetes also involves two parts:</p> <ul> <li>Delete related database objects and tablespace with PostgreSQL,</li> <li>Delete tablespace storage in Kubernetes.</li> </ul> <p>To make tablespace deletion with PostgreSQL possible, you should make this tablespace empty (it is impossible to drop a tablespace until all objects in all databases using this tablespace have been removed). Tablespaces are listed in the <code>pg_tablespace</code> table, and you can use it to find out which objects are stored in a specific tablespace. The example command for the <code>lake</code> tablespace will look as follows:</p> <pre><code>SELECT relname FROM pg_class WHERE reltablespace=(SELECT oid FROM pg_tablespace WHERE spcname='lake');\n</code></pre> <p>When your tablespace is empty, you can log in to the PostgreSQL Primary instance as a superuser, and then execute the <code>DROP TABLESPACE &lt;tablespace_name&gt;;</code> command.</p> <p>Now, when the PostgreSQL part is finished, you can remove the tablespace entry from the <code>tablespaceStorages</code> section (don\u2019t forget to run the <code>kubectl apply -f deploy/cr.yaml</code> command to apply changes).</p> <p>However, Persistent Volumes will still be mounted to the <code>/tablespaces</code> directory in PostgreSQL Pods. To remove these mounts, you should edit all Deployment objects for <code>pgPrimary</code> and <code>pgReplica</code> instances in your Kubernetes cluster and remove the <code>Volume</code> and <code>VolumeMount</code> entries related to your tablespace.</p> <p>You can see the list of Deployment objects with the <code>kubectl get deploy</code> command. Running it for a default cluster named <code>cluster1</code> results in the following output:</p> <pre><code>NAME                            READY   UP-TO-DATE   AVAILABLE   AGE\ncluster1                        1/1     1            1           156m\ncluster1-backrest-shared-repo   1/1     1            1           156m\ncluster1-pgbouncer              3/3     3            3           154m\ncluster1-repl1                  1/1     1            1           154m\ncluster1-repl2                  1/1     1            1           154m\npostgres-operator               1/1     1            1           157m\n</code></pre> <p>Now run <code>kubectl edit deploy &lt;oblect_name&gt;</code> for <code>cluster1</code>, <code>cluster1-repl1</code>, and <code>cluster1-repl2</code> objects consequently. Each command will open a text editor, where you should remove the appropriate lines, which in case of the <code>lake</code> tablespace will look as follows:</p> <pre><code>...\nspec:\n    ...\n    containers:\n      - name: database\n        ...\n        volumeMounts:\n          - name: tablespace-lake\n            mountPath: /tablespaces/lake\n    volumes:\n      ...\n      - name: tablespace-lake\n        persistentVolumeClaim:\n          claimName: cluster1-tablespace-lake\n      ...\n</code></pre> <p>Finishing the edit causes Pods to be recreated without tablespace mounts.</p>"},{"location":"tablespace.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"telemetry.html","title":"Telemetry","text":"<p>The Telemetry function enables the Operator gathering and sending basic anonymous data to Percona, which helps us to determine where to focus the development and what is the uptake for each release of Operator. </p> <p>The following information is gathered:</p> <ul> <li>ID of the Custom Resource (the <code>metadata.uid</code> field)</li> <li>Kubernetes version</li> <li>Platform (is it Kubernetes or Openshift)</li> <li>PMM Version</li> <li>Operator version</li> <li>PostgreSQL version</li> <li>PgBackRest version</li> </ul> <p>We do not gather anything that identify a system, but the following thing should be mentioned: Custom Resource ID is a unique ID generated by Kubernetes for each Custom Resource.</p> <p>Telemetry is enabled by default and is sent to the Version Service server - the same server that the Operator uses to obtain fresh information about version numbers and valid image paths needed for the upgrade.</p> <p>The landing page for this service, check.percona.com, explains what this service is.</p> <p>You can disable telemetry with a special option when installing the Operator:</p> <ul> <li>if you install the Operator with helm, use the following installation command:</li> </ul> <pre><code>$ helm install my-db percona/pg-db --version {{ release }} --namespace my-namespace --set disable_telemetry=\"true\"\n</code></pre> <ul> <li>if you don\u2019t use helm for installation, you have to edit the <code>operator.yaml</code>   before applying it with the <code>kubectl apply -f deploy/operator.yaml</code> command.   Open the <code>operator.yaml</code> file with your text editor, find the   <code>disable_telemetry</code> key and set it to <code>true</code>:</li> </ul> <pre><code>...\ndisable_telemetry: \"true\"\n...\n</code></pre>"},{"location":"telemetry.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"trademark-policy.html","title":"Trademark policy","text":"<p>This Trademark Policy is to ensure that users of Percona-branded products or services know that what they receive has really been developed, approved, tested and maintained by Percona. Trademarks help to prevent confusion in the marketplace, by distinguishing one company\u2019s or person\u2019s products and services from another\u2019s.</p> <p>Percona owns a number of marks, including but not limited to Percona, XtraDB, Percona XtraDB, XtraBackup, Percona XtraBackup, Percona Server, and Percona Live, plus the distinctive visual icons and logos associated with these marks. Both the unregistered and registered marks of Percona are protected.</p> <p>Use of any Percona trademark in the name, URL, or other identifying characteristic of any product, service, website, or other use is not permitted without Percona\u2019s written permission with the following three limited exceptions.</p> <p>First, you may use the appropriate Percona mark when making a nominative fair use reference to a bona fide Percona product.</p> <p>Second, when Percona has released a product under a version of the GNU General Public License (\u201cGPL\u201d), you may use the appropriate Percona mark when distributing a verbatim copy of that product in accordance with the terms and conditions of the GPL.</p> <p>Third, you may use the appropriate Percona mark to refer to a distribution of GPL-released Percona software that has been modified with minor changes for the sole purpose of allowing the software to operate on an operating system or hardware platform for which Percona has not yet released the software, provided that those third party changes do not affect the behavior, functionality, features, design or performance of the software. Users who acquire this Percona-branded software receive substantially exact implementations of the Percona software.</p> <p>Percona reserves the right to revoke this authorization at any time in its sole discretion. For example, if Percona believes that your modification is beyond the scope of the limited license granted in this Policy or that your use of the Percona mark is detrimental to Percona, Percona will revoke this authorization. Upon revocation, you must immediately cease using the applicable Percona mark. If you do not immediately cease using the Percona mark upon revocation, Percona may take action to protect its rights and interests in the Percona mark. Percona does not grant any license to use any Percona mark for any other modified versions of Percona software; such use will require our prior written permission.</p> <p>Neither trademark law nor any of the exceptions set forth in this Trademark Policy permit you to truncate, modify or otherwise use any Percona mark as part of your own brand. For example, if XYZ creates a modified version of the Percona Server, XYZ may not brand that modification as \u201cXYZ Percona Server\u201d or \u201cPercona XYZ Server\u201d, even if that modification otherwise complies with the third exception noted above.</p> <p>In all cases, you must comply with applicable law, the underlying license, and this Trademark Policy, as amended from time to time. For instance, any mention of Percona trademarks should include the full trademarked name, with proper spelling and capitalization, along with attribution of ownership to Percona Inc. For example, the full proper name for XtraBackup is Percona XtraBackup. However, it is acceptable to omit the word \u201cPercona\u201d for brevity on the second and subsequent uses, where such omission does not cause confusion.</p> <p>In the event of doubt as to any of the conditions or exceptions outlined in this Trademark Policy, please contact trademarks@percona.com for assistance and we will do our very best to be helpful.</p>"},{"location":"trademark-policy.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"update.html","title":"Update Percona Operator for PostgreSQL","text":"<p>Percona Operator for PostgreSQL allows upgrades to newer versions. This includes upgrades of the Operator itself, and upgrades of the Percona Distribution for PostgreSQL.</p>"},{"location":"update.html#upgrading-the-operator","title":"Upgrading the Operator","text":"<p>Only the incremental update to a nearest minor version of the Operator 1.x is supported. To update to a newer version, which differs from the current version by more than one, make several incremental updates sequentially. See documentation archive for documentation on previous versions of the Operator.</p> <p>You can check the Operator images to find out the current Operator version with the following command (in case it is deployed in the <code>pgo</code> namespace):</p> <pre><code>$ kubectl get deployment postgres-operator -o yaml | grep percona-postgresql-operator\n</code></pre> Expected output <pre><code>image: percona/percona-postgresql-operator:1.4.0-pgo-apiserver\nimage: percona/percona-postgresql-operator:1.4.0-postgres-operator\nimage: percona/percona-postgresql-operator:1.4.0-pgo-scheduler\nimage: percona/percona-postgresql-operator:1.4.0-pgo-event\n</code></pre> <p>Note</p> <p>The above command and other commands in this section follow the assumption that the context with the Operator namespace (<code>pgo</code> by default) was set. You can set context as follows:</p> <pre><code>$ kubectl config set-context $(kubectl config current-context) --namespace=pgo\n</code></pre> <p>Alternatively, you can specify the proper namespace explicitly: for example, by adding the <code>-n pgo</code> option to <code>kubectl</code> in all commands.</p> <p>The following steps will update the Operator to a newer version:</p> <ol> <li> <p>Check that the Operator deployment job is not still present in your cluster:</p> <pre><code>$ kubectl get job/pgo-deploy -n pgo\n</code></pre> Expected output <pre><code>NAME         COMPLETIONS   DURATION   AGE\npgo-deploy   1/1           81s        5m53s\n</code></pre> <p>If the job is not present, you will get a message that it is not found. Otherwise you should delete this job before upgrading the Operator:</p> <pre><code>$ kubectl delete  job/pgo-deploy -n pgo\n</code></pre> </li> <li> <p>Upgrading the Operator is similar to deploying a new Operator version, but     you should change the <code>DEPLOY_ACTION</code> option in the <code>deploy/operator.yaml</code>     file before applying it from <code>install</code> to <code>update</code>:</p> <pre><code>...\n  containers:\n    - name: pgo-deploy\n      image: percona/percona-postgresql-operator:1.4.0-pgo-deployer\n      imagePullPolicy: Always\n      env:\n        - name: DEPLOY_ACTION\n          value: update\n...\n</code></pre> <p>You can automate this with the yq tool as follows, assuming that you are upgrading to the Operator version {{ release }}:</p> <pre><code>$ curl -s https://raw.githubusercontent.com/percona/percona-postgresql-operator/v{{ release }}/deploy/operator.yaml | yq w --doc 4 - \"spec.template.spec.containers[0].env[0].value\" \"update\" | kubectl apply -f -\n$ kubectl wait --for=condition=Complete job/pgo-deploy --timeout=90s\n</code></pre> <p>Note</p> <p>The example above (and other examples in this document) uses the yq version 3.4.0. Note that the syntax for the <code>yq</code> command may be slightly different in other versions.</p> <p>Applying the modified <code>operator.yaml</code> will produce the command output as follows:</p> <pre><code>serviceaccount/pgo-deployer-sa unchanged\nclusterrole.rbac.authorization.k8s.io/pgo-deployer-cr unchanged\nconfigmap/pgo-deployer-cm configured\nclusterrolebinding.rbac.authorization.k8s.io/pgo-deployer-crb unchanged\njob.batch/pgo-deploy created\n</code></pre> </li> <li> <p>The <code>pgo-deploy</code> Kubernetes Job created to carry on the Operator deployment     process can take a minute or more to be completed. You can track it with the     following command:</p> <pre><code>$ kubectl get job/pgo-deploy\n</code></pre> Expected output <pre><code>NAME         COMPLETIONS   DURATION   AGE\npgo-deploy   1/1           81s        5m53s\n</code></pre> <p>When it reaches the COMPLETIONS count of <code>1/1</code>, you can safely delete the job as follows:</p> <pre><code>$ kubectl delete  job/pgo-deploy\n</code></pre> <p>Note</p> <p>Deleting the <code>pgo-deploy</code> job will be needed before the next upgrade of the Operator.</p> </li> </ol>"},{"location":"update.html#upgrading-percona-distribution-for-postgresql","title":"Upgrading Percona Distribution for PostgreSQL","text":""},{"location":"update.html#automatic-upgrade","title":"Automatic upgrade","text":"<p>Starting from version 1.1.0, the Operator does fully automatic upgrades to the newer versions of Percona PostgreSQL Cluster within the method named Smart Updates.</p> <p>The Operator will carry on upgrades according to the following algorithm. It will query a special Version Service server at scheduled times to obtain fresh information about version numbers and valid image paths needed for the upgrade. If the current version should be upgraded, the Operator updates the CR to reflect the new image paths and carries on sequential Pods deletion in a safe order, allowing the cluster Pods to be re-deployed with the new image.</p> <p>Note</p> <p>Version Service is in technical preview status and is disabled by default for the Operator version 1.1.0. Disabling Version Service makes Smart Updates rely on the <code>image</code> keys in the Operator\u2019s Custom Resource.</p> <p>The upgrade details are set in the <code>upgradeOptions</code> section of the <code>deploy/cr.yaml</code> configuration file. Make the following edits to configure updates:</p> <ol> <li> <p>Set the <code>apply</code> option to one of the following values:</p> <ul> <li><code>recommended</code> - automatic upgrades will choose the most recent version     of software flagged as recommended (for clusters created from scratch,     the Percona Distribution for PostgreSQL 14 version will be selected     instead of the Percona Distribution for PostgreSQL 13 or 12 version     regardless of the image path; for already existing clusters, 14 vs. 13 or     12 branch choice will be preserved),</li> <li><code>14-recommended</code>, <code>13-recommended</code>, <code>12-recommended</code> - same as above,     but preserves specific major Percona Distribution for PostgreSQL version     for newly provisioned clusters (for example, 14 will not be automatically     used instead of 13),</li> <li><code>latest</code> - automatic upgrades will choose the most recent version of     the software available,</li> <li><code>14-latest</code>, <code>13-latest</code>, <code>12-latest</code> - same as above, but preserves     specific major Percona Distribution for PostgreSQL version for newly     provisioned clusters (for example, 14 will not be automatically     used instead of 13),</li> <li>version number - specify the desired version explicitly,</li> <li><code>never</code> or <code>disabled</code> - disable automatic upgrades</li> </ul> <p>Note</p> <p>When automatic upgrades are disabled by the <code>apply</code> option, Smart Update functionality will continue working for changes triggered by other events, such as updating a ConfigMap, rotating a password, or changing resource values.</p> </li> <li> <p>Make sure the <code>versionServiceEndpoint</code> key is set to a valid Version     Server URL (otherwise Smart Updates will not occur).</p> Percona\u2019s Version ServiceVersion Service inside your cluster <p>You can use the URL of the official Percona\u2019s Version Service (default).  Set <code>versionServiceEndpoint</code> to <code>https://check.percona.com</code>.</p> <p>Alternatively, you can run Version Service inside your cluster. This can be done with the <code>kubectl</code> command as follows:</p> <pre><code>$ kubectl run version-service --image=perconalab/version-service --env=\"SERVE_HTTP=true\" --port 11000 --expose\n</code></pre> <p>Note</p> <p>Version Service is never checked if automatic updates are disabled. If automatic updates are enabled, but Version Service URL can not be reached, upgrades will not occur.</p> </li> <li> <p>Use the <code>schedule</code> option to specify the update checks time in CRON format.</p> <p>The following example sets the midnight update checks with the official Percona\u2019s Version Service:</p> <pre><code>spec:\n  upgradeOptions:\n    apply: recommended\n    versionServiceEndpoint: https://check.percona.com\n    schedule: \"0 4 * * *\"\n...\n</code></pre> </li> </ol>"},{"location":"update.html#semi-automatic-upgrade","title":"Semi-automatic upgrade","text":"<p>The following command will allow you to update the Operator to current version (use the name of your cluster instead of the <code>&lt;cluster-name&gt;</code> placeholder).</p> <pre><code>$ kubectl patch perconapgcluster/&lt;cluster-name&gt; --type json -p '[{\"op\": \"replace\", \"path\": \"/spec/backup/backrestRepoImage\", \"value\": \"percona/percona-postgresql-operator:{{ release }}-ppg14-pgbackrest-repo\"},{\"op\":\"replace\",\"path\":\"/spec/backup/image\",\"value\":\"percona/percona-postgresql-operator:{{ release }}-ppg14-pgbackrest\"},{\"op\":\"replace\",\"path\":\"/spec/pgBadger/image\",\"value\":\"percona/percona-postgresql-operator:{{ release }}-ppg14-pgbadger\"},{\"op\":\"replace\",\"path\":\"/spec/pgBouncer/image\",\"value\":\"percona/percona-postgresql-operator:{{ release }}-ppg14-pgbouncer\"},{\"op\":\"replace\",\"path\":\"/spec/pgPrimary/image\",\"value\":\"percona/percona-postgresql-operator:{{ release }}-ppg14-postgres-ha\"},{\"op\":\"replace\",\"path\":\"/spec/userLabels/pgo-version\",\"value\":\"{{ release }}\"},{\"op\":\"replace\",\"path\":\"/metadata/labels/pgo-version\",\"value\":\"{{ release }}\"}]'\n</code></pre> <p>Note</p> <p>The above example is composed in assumption of using PostgreSQL 14 as a database management system. For PostgreSQL 13 you should change all occurrences of the <code>ppg14</code> substring to <code>ppg13</code>.</p> <p>This will carry on the image and the cluster version update.</p>"},{"location":"update.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"users.html","title":"Users","text":"<p>User accounts within the Cluster can be divided into two different groups:</p> <ul> <li>application-level users: the unprivileged user accounts,</li> <li>system-level users: the accounts needed to automate the cluster deployment and management tasks.</li> </ul>"},{"location":"users.html#system-users","title":"System Users","text":"<p>Credentials for system users are stored as a Kubernetes Secrets object. The Operator requires to be deployed before PostgreSQL Cluster is started. The name of the required secrets (<code>cluster1-users</code> by default) should be set in the <code>spec.secretsName</code> option of the <code>deploy/cr.yaml</code> configuration file.</p> <p>The following table shows system users\u2019 names and purposes.</p> <p>Warning</p> <p>These users should not be used to run an application.</p> <p>The default PostgreSQL instance installation via the Percona Operator for PostgreSQL comes with the following users:</p> Role name Attributes <code>postgres</code> Superuser, Create role, Create DB, Replication, Bypass RLS <code>primaryuser</code> Replication <code>pguser</code> Non-privileged user <code>pgbouncer</code> Administrative user for the pgBouncer connection pooler <p>The <code>postgres</code> user will be the admin user for the database instance. The <code>primaryuser</code> is used for replication between primary and replicas. The <code>pguser</code> is the default non-privileged user (you can configure different name of this user in the <code>spec.user</code>  Custom Resource option).</p>"},{"location":"users.html#yaml-object-format","title":"YAML Object Format","text":"<p>The default name of the Secrets object for these users is <code>cluster1-users</code> and can be set in the CR for your cluster in <code>spec.secretsName</code> to something different. When you create the object yourself, it should match the following simple format:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: cluster1-users\ntype: Opaque\nstringData:\n  pgbouncer: pgbouncer_password\n  postgres: postgres_password\n  primaryuser: primaryuser_password\n pguser: pguser_password\n</code></pre> <p>The example above matches what is shipped in the deploy/secrets.yaml file.</p> <p>As you can see, we use the <code>stringData</code> type when creating the Secrets object, so all values for each key/value pair are stated in plain text format convenient from the user\u2019s point of view. But the resulting Secrets object contains passwords stored as <code>data</code> - i.e., base64-encoded strings. If you want to update any field, you\u2019ll need to encode the value into base64 format. To do this, you can run <code>echo -n \"password\" | base64 --wrap=0</code> (or just  <code>echo -n \"password\" | base64</code> in case of Apple macOS) in your local shell to get valid values. For example, setting the PMM Server user\u2019s password to <code>new_password</code> in the <code>cluster1-users</code> object can be done with the following command:</p> in Linuxin macOS <pre><code>$ kubectl patch secret/cluster1-users -p '{\"data\":{\"pguser\": \"'$(echo -n new_password | base64 --wrap=0)'\"}}'\n</code></pre> <pre><code>$ kubectl patch secret/cluster1-users -p '{\"data\":{\"pguser\": \"'$(echo -n new_password | base64)'\"}}'\n</code></pre>"},{"location":"users.html#application-users","title":"Application users","text":"<p>By default you can connect to PostgreSQL as non-privileged <code>pguser</code> user. Also, you can login as <code>postgres</code> (the superuser) to PostgreSQL Pods, but pgBouncer (the connection pooler for PostgreSQL) doesn\u2019t allow <code>postgres</code> user access by default. That\u2019s done for security reasons.</p> <p>If you still need to provide <code>postgres</code> user access to PostgreSQL instances from the outside, set the <code>pgBouncer.exposePostgresUser</code> option in the <code>deploy/cr.yaml</code> configuration file to <code>true</code> and apply changes as usual by the <code>kubectl apply -f deploy/cr.yaml</code> command.</p> <p>Note</p> <p>Allowing superusers to access to the cluster is not recommended.</p>"},{"location":"users.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/index.html","title":"Percona Operator for PostgreSQL Release Notes","text":"<ul> <li> <p>Percona Ooerator for PostgreSQL 1.5.1 (2024-01-29)</p> </li> <li> <p>Percona Ooerator for PostgreSQL 1.5.0 (2023-12-11)</p> </li> <li> <p>Percona Operator for PostgreSQL 1.4.0 (2023-03-31)</p> </li> <li> <p>Percona Operator for PostgreSQL 1.3.0 (2022-08-04)</p> </li> <li> <p>Percona Operator for PostgreSQL 1.2.0 (2022-04-06)</p> </li> <li> <p>Percona Distribution for PostgreSQL Operator 1.1.0 (2021-12-07)</p> </li> <li> <p>Percona Distribution for PostgreSQL Operator 1.0.0 (2021-10-07)</p> </li> <li> <p>Percona Distribution for PostgreSQL Operator 0.2.0 (2021-08-12)</p> </li> <li> <p>Percona Distribution for PostgreSQL Operator 0.1.0 (2021-05-10)</p> </li> </ul>"},{"location":"ReleaseNotes/index.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN0.1.0.html","title":"Percona Distribution for PostgreSQL Operator 0.1.0","text":"<ul> <li> <p>Date</p> <p>May 10, 2021</p> </li> <li> <p>Installation</p> <p>Installing Percona Distribution for PostgreSQL Operator</p> </li> </ul> <p>The Percona Operator is based on best practices for configuration and setup of a Percona Distribution for PostgreSQL on Kubernetes. The benefits of the Operator are many, but saving time and delivering a consistent and vetted environment is key.</p> <p>Kubernetes provides users with a distributed orchestration system that automates the deployment, management, and scaling of containerized applications. The Operator extends the Kubernetes API with a new custom resource for deploying, configuring, and managing the application through the whole life cycle. You can compare the Kubernetes Operator to a System Administrator who deploys the application and watches the Kubernetes events related to it, taking administrative/operational actions when needed.</p> <p>Version 0.1.0 of the Percona Distribution for PostgreSQL Operator is a tech preview release and it is not recommended for production environments.</p> <p>You can install Percona Distribution for PostgreSQL Operator on Kubernetes, Google Kubernetes Engine (GKE), and Amazon Elastic Kubernetes Service (EKS) clusters. The Operator is based on Postgres Operator developed by Crunchy Data.</p> <p>Here are the main differences between v 0.1.0 and the original Operator:</p> <ul> <li> <p>Percona Distribution for PostgreSQL is now used as the main container image.</p> </li> <li> <p>It is possible to specify custom images for all components separately. For example, users can easily build and use custom images for one or several components (e.g. pgBouncer) while all other images will be the official ones. Also, users can build and use all custom images.</p> </li> <li> <p>All container images are reworked and simplified. They are built on Red Hat Universal Base Image (UBI) 8.</p> </li> <li> <p>The Operator has built-in integration with Percona Monitoring and Management v2.</p> </li> <li> <p>A build/test infrastructure was created, and we have started adding e2e tests to be sure that all pieces of the cluster work together as expected.</p> </li> <li> <p>We have phased out the <code>pgo</code> CLI tool, and the Custom Resource UX will be completely aligned with other Percona Operators in the following release.</p> </li> </ul> <p>Once Percona Operator is promoted to GA, users would be able to get the full package of services from Percona teams.</p> <p>While the Operator is in its very first release, instructions on how to install and configure it are already available along with the source code hosted in our Github repository.</p> <p>Help us improve our software quality by reporting any bugs you encounter using our bug tracking system.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN0.1.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN0.2.0.html","title":"Percona Distribution for PostgreSQL Operator 0.2.0","text":"<ul> <li> <p>Date</p> <p>August 12, 2021</p> </li> <li> <p>Installation</p> <p>Installing Percona Distribution for PostgreSQL Operator</p> </li> </ul> <p>Version 0.2.0 of the Percona Distribution for PostgreSQL Operator is a Beta release, and it is not recommended for production environments.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN0.2.0.html#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>{{ k8spgjira(80) }}: The Custom Resource structure was reworked to provide the same look and feel as in other Percona Operators. Read more about Custom Resource options in the documentation and review the default  <code>deploy/cr.yaml</code> configuration file on GitHub.</p> </li> <li> <p>{{ k8spgjira(53) }}: Merged upstream CrunchyData Operator v4.7.0 made it possible to use Google Cloud Storage as an object store for backups without using third-party tools</p> </li> <li> <p>{{ k8spgjira(42) }}: There is no need to specify the name of the pgBackrest Pod in the backup manifest anymore as it is detected automatically by the Operator</p> </li> <li> <p>{{ k8spgjira(30) }}: Replicas management is now performed through a main Custom Resource manifest instead of creating separate Kubernetes resources. This also adds the possibility of scaling up/scaling down replicas via the \u2018deploy/cr.yaml\u2019 configuration file</p> </li> <li> <p>{{ k8spgjira(66) }}: Helm chart is now officially provided with the Operator</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN0.2.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.0.0.html","title":"Percona Distribution for PostgreSQL Operator 1.0.0","text":"<ul> <li> <p>Date</p> <p>October 7, 2021</p> </li> <li> <p>Installation</p> <p>Installing Percona Distribution for PostgreSQL Operator</p> </li> </ul> <p>Percona announces the general availability of Percona Distribution for PostgreSQL Operator 1.0.0.</p> <p>The Percona Distribution for PostgreSQL Operator automates the lifecycle, simplifies deploying and managing open source PostgreSQL clusters on Kubernetes.</p> <p>The Operator follows best practices for configuration and setup of the Percona Distribution for PostgreSQL. The Operator provides a consistent way to package, deploy, manage, and perform a backup and a restore for a Kubernetes application. Operators deliver automation advantages in cloud-native applications.</p> <p>The advantages are the following:</p> <ul> <li> <p>Deploy a Percona Distribution for PostgreSQL with no single point of failure and environment which can span multiple availability zones</p> </li> <li> <p>Modify the Percona Distribution for PostgreSQL size parameter to add or remove PostgreSQL instances</p> </li> <li> <p>Use single Custom Resource as a universal entry point to configure the cluster, similar to other Percona Operators</p> </li> <li> <p>Carry on semi-automatic upgrades of the Operator and PostgreSQL to newer versions</p> </li> <li> <p>Integrate with Percona Monitoring and Management (PMM) to seamlessly monitor your Percona Distribution for PostgreSQL</p> </li> <li> <p>Automate backups or perform on-demand backups as needed with support for performing an automatic restore</p> </li> <li> <p>Use cloud storage with S3-compatible APIs or Google Cloud for backups</p> </li> <li> <p>Use Transport Layer Security (TLS) for the replication and client traffic</p> </li> <li> <p>Support advanced Kubernetes features such as pod disruption budgets, node selector, constraints, tolerations, priority classes, and affinity/anti-affinity</p> </li> </ul> <p>Percona Distribution for PostgreSQL Operator is based on Postgres Operator developed by Crunchy Data.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.0.0.html#release-highlights","title":"Release Highlights","text":"<ul> <li> <p>It is now possible to configure scheduled backups following the declarative approach in the <code>deploy/cr.yaml</code> file, similar to other Percona Kubernetes Operators</p> </li> <li> <p>OpenShift compatibility allows running Percona Distribution for PostgreSQL on Red Hat OpenShift Container Platform</p> </li> <li> <p>For the first time, the main functionality of the Operator is covered by functional tests, which ensure the overall quality and stability</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.0.0.html#new-features-and-improvements","title":"New Features and Improvements","text":"<ul> <li> <p>{{ k8spgjira(96) }}: PMM Client container does not cause the crash of the whole database Pod if <code>pmm-agent</code> is not working properly</p> </li> <li> <p>{{ k8spgjira(86) }}: The Operator is now compatible with the OpenShift platform</p> </li> <li> <p>{{ k8spgjira(62) }}: Configuring scheduled backups through the main Custom Resource is now supported</p> </li> <li> <p>{{ k8spgjira(99) }}, {{ k8spgjira(131) }}: The Operator documentation was substantially improved, and now it covers among other things the usage of Transport Layer Security (TLS) for internal and external communications, and cluster upgrades</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.0.0.html#supported-platforms","title":"Supported Platforms","text":"<p>The following platforms were tested and are officially supported by Operator 1.0.0:</p> <ul> <li> <p>OpenShift 4.6 - 4.8</p> </li> <li> <p>Google Kubernetes Engine (GKE) 1.17 - 1.21</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.21</p> </li> </ul> <p>This list only includes the platforms that the Operator is specifically tested on as a part of the release process. Other Kubernetes flavors and versions depend on the backward compatibility offered by Kubernetes itself.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.0.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html","title":"Percona Distribution for PostgreSQL Operator 1.1.0","text":"<ul> <li> <p>Date</p> <p>December 7, 2021</p> </li> <li> <p>Installation</p> <p>Installing Percona Distribution for PostgreSQL Operator</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html#release-highlights","title":"Release Highlights","text":"<ul> <li> <p>A Kubernetes-native horizontal scaling capability was added to the Custom Resource to unblock Horizontal Pod Autoscaler and Kubernetes Event-driven Autoscaling (KEDA) usage</p> </li> <li> <p>The Smart Upgrade functionality along with the technical preview of the Version Service allows users to automatically get the latest version of the software compatible with the Operator and apply it safely</p> </li> <li> <p>Percona Distribution for PostgreSQL Operator now supports PostgreSQL 14</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html#new-features","title":"New Features","text":"<ul> <li> <p>{{ k8spgjira(101) }}: Add support for Kubernetes horizontal scaling to set the number of Replicas dynamically via the <code>kubectl scale</code> command or Horizontal Pod Autoscaler</p> </li> <li> <p>{{ k8spgjira(77) }}: Add support for PostgreSQL 14 in the Operator</p> </li> <li> <p>{{ k8spgjira(75) }}: Manage Operator\u2019s system users hrough a single Secret resource even after cluster creation</p> </li> <li> <p>{{ k8spgjira(71) }}: Add Smart Upgrade functionality to automate Percona Distribution for PostgreSQL upgrades</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html#improvements","title":"Improvements","text":"<ul> <li>{{ k8spgjira(96) }}: PMM container does not cause the crash of the whole database Pod if pmm-agent is not working properly</li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li>{{ k8spgjira(120) }}: The Operator default behavior is now to keep backups and PVCs when the cluster is deleted</li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html#supported-platforms","title":"Supported platforms","text":"<p>The following platforms were tested and are officially supported by the Operator 1.1.0:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.19 - 1.22</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.18 - 1.21</p> </li> <li> <p>OpenShift 4.7 - 4.9</p> </li> </ul> <p>This list only includes the platforms that the Percona Operators are specifically tested on as part of the release process. Other Kubernetes flavors and versions depend on the backward compatibility offered by Kubernetes itself.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.1.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html","title":"Percona Operator for PostgreSQL 1.2.0","text":"<ul> <li> <p>Date</p> <p>April 6, 2022</p> </li> <li> <p>Installation</p> <p>Percona Operator for PostgreSQL</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html#release-highlights","title":"Release Highlights","text":"<ul> <li> <p>With this release, the Operator turns to a simplified naming convention and changes its official name to Percona Operator for PostgreSQL</p> </li> <li> <p>Starting from this release, the Operator automatically generates TLS certificates and turns on encryption by default at cluster creation time. This includes both external certificates which allow users to connect to pgBouncer and PostgreSQL via the encrypted channel, and internal ones used for communication between PostgreSQL cluster nodes</p> </li> <li> <p>Various cleanups in the deploy/cr.yaml configuration file simplify the deployment of the cluster, making no need in going into YAML manifests and tuning them</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html#improvements","title":"Improvements","text":"<ul> <li> <p>{{ k8spgjira(149) }}: It is now possible to explicitly set the version of PostgreSQL for newly provisioned clusters. Before that, all new clusters were started with the latest PostgreSQL version if Version Service was enabled</p> </li> <li> <p>{{ k8spgjira(148) }}: Add possibility of specifying <code>imagePullPolicy</code> option for all images in the Custom Resource of the cluster to run in air-gapped environments</p> </li> <li> <p>{{ k8spgjira(147) }}: Users now can pass additional customizations to pgBackRest with the  pgBackRest configuration options provided via ConfigMap</p> </li> <li> <p>{{ k8spgjira(142) }}: Introduce deploy/cr-minimal.yaml configuration file to deploy minimal viable clusters - useful for developers to deploy PostgreSQL on local Kubernetes clusters, such as Minikube</p> </li> <li> <p>{{ k8spgjira(141) }}: YAML manifest cleanup simplifies cluster deployment, reducing it to just two commands</p> </li> <li> <p>{{ k8spgjira(112) }}: Enable automated generation of TLS certificates and provide encryption for all new clusters by default</p> </li> <li> <p>{{ k8spgjira(161) }}: The Operator documentation now has a how-to that covers deploying a standby PostgreSQL cluster on Kubernetes</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>{{ k8spgjira(115) }}: Fix the bug that caused creation a \u201ccloned\u201d cluster with <code>pgDataSource</code> to fail due to missing Secrets</p> </li> <li> <p>{{ k8spgjira(163) }}: Fix the security vulnerability CVE-2021-40346 by removing the unused dependency in the Operator images</p> </li> <li> <p>{{ k8spgjira(152) }}: Fix the bug that prevented deploying the Operator in disabled/readonly namespace mode. It is now possible to deploy several operators in different namespaces in the same cluster</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html#options-changes","title":"Options Changes","text":"<ul> <li>{{ k8spgjira(116) }}: The <code>backrest-restore-from-cluster</code> parameter was renamed to <code>backrest-restore-cluster</code> for clarity in the deploy/backup/restore.yaml file used to restore the cluster from a previously saved backup</li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html#supported-platforms","title":"Supported platforms","text":"<p>The following platforms were tested and are officially supported by the Operator 1.2.0:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.19 - 1.22</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.19 - 1.21</p> </li> <li> <p>OpenShift 4.7 - 4.9</p> </li> </ul> <p>This list only includes the platforms that the Percona Operators are specifically tested on as part of the release process. Other Kubernetes flavors and versions depend on the backward compatibility offered by Kubernetes itself.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.2.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.3.0.html","title":"Percona Operator for PostgreSQL 1.3.0","text":"<ul> <li> <p>Date</p> <p>August 4, 2022</p> </li> <li> <p>Installation</p> <p>Percona Operator for PostgreSQL</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.3.0.html#release-highlights","title":"Release Highlights","text":"<ul> <li> <p>The automated upgrade is now disabled by default to prevent an unplanned downtimes for user applications and to provide defaults more focused on strict user\u2019s control over the cluster</p> </li> <li> <p>Flexible anti-affinity configuration is now available, which allows the Operator to isolate PostgreSQL cluster instances on different Kubernetes nodes or to increase its availability by placing PostgreSQL instances in different availability zones</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.3.0.html#improvements","title":"Improvements","text":"<ul> <li> <p>{{ k8spgjira(155) }}: Flexible anti-affinity configuration</p> </li> <li> <p>{{ k8spgjira(196) }}: Add possibility for postgres user to connect to PostgreSQL through PgBouncer with a new <code>pgBouncer.exposePostgresUser</code> Custom Resource option</p> </li> <li> <p>{{ k8spgjira(218) }}: The automated upgrade is now disabled by default to prevent an unplanned downtimes for user applications and to provide defaults more focused on strict user\u2019s contol over the cluster; also the user is now able to turn off sending data to the Version Service server</p> </li> <li> <p>{{ k8spgjira(226) }}: A new build and testing guide allows user to easily experiment with the source code of the Operator</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.3.0.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>{{ k8spgjira(178) }}: Fix the bug in the instruction on passing custom configuration options for PostgreSQL which made it usable for the new cluster only</p> </li> <li> <p>{{ k8spgjira(193) }}: Fix the bug which caused the Operator crash without pgReplicas section in Custom Resource definition</p> </li> <li> <p>{{ k8spgjira(197) }}: Fix the bug which caused the Operator to make connection requests to Version Service even with disabled Smart Update</p> </li> <li> <p>{{ k8spgjira(207) }}: Fix the bug due to which restoring S3 backup from storage with self-signed certificates didn\u2019t work, by introducing the special <code>backup.storages.verifyTLS</code> option to address this issue</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.3.0.html#supported-platforms","title":"Supported platforms","text":"<p>The following platforms were tested and are officially supported by the Operator 1.3.0:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.21 - 1.24</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.20 - 1.22</p> </li> <li> <p>OpenShift 4.7 - 4.10</p> </li> </ul> <p>This list only includes the platforms that the Percona Operators are specifically tested on as part of the release process. Other Kubernetes flavors and versions depend on the backward compatibility offered by Kubernetes itself.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.3.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.4.0.html","title":"Percona Operator for PostgreSQL 1.4.0","text":"<ul> <li> <p>Date</p> <p>March 31, 2023</p> </li> <li> <p>Installation</p> <p>Percona Operator for PostgreSQL</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.4.0.html#improvements","title":"Improvements","text":"<ul> <li> <p>{{ k8spgjira(188) }}: Add Custom Resource options to set static IP address for the pgPrimary, pgReplicas, and pgBouncer LoadBalancers</p> </li> <li> <p>{{ k8spgjira(269) }}: It is now possible to define affinity and anti-affinity rules for backup Pods</p> </li> <li> <p>{{ k8spgjira(270) }}: The new <code>schedule.backrestOpts</code> Custom Resource option allows customizing pgBackRest parameters for scheduled backups</p> </li> <li> <p>{{ k8spgjira(292) }}: The Operator now uses units based on the power of 2 (e.g. <code>GiB</code> instead of <code>G</code>) for the storage size, to make it multiple of the 1024 default kernel block size (thanks to Rodney Karemba for contribution)</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.4.0.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>{{ k8spgjira(286) }}: Fix a bug which caused PMM client connection fail when the <code>TLSOnly</code> Custom Resource option was set to require TLS for all connections</p> </li> <li> <p>{{ k8spgjira(290) }}: Fix a bug due to which ssh connection used for backups and new replica creation could hang if exceeding the PostgreSQL 60 seconds timeout (e.g. because of network problems); to avoid such orphaned connections, gbackrest archive-push command is now automatically killed after timeout</p> </li> <li> <p>{{ k8spgjira(291) }}: Fix a bug which prevented backup schedule in the Custom Resource to be updated  without deleting the existing schedule first and recreating it as a new one</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.4.0.html#supported-platforms","title":"Supported platforms","text":"<p>The Operator was developed and tested with PostgreSQL versions 12.14, 13.10, and 14.7. Other options may also work but have not been tested. The Operator 1.4.0 provides connection pooling based on pgBouncer 1.18.0 and high-availability implementation based on Patroni 2.1.4.</p> <p>The following platforms were tested and are officially supported in this release:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.22 - 1.25</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.22 - 1.25</p> </li> <li> <p>OpenShift 4.10 - 4.12</p> </li> <li> <p>Minikube 1.28 (based on Kubernetes 1.25)</p> </li> </ul> <p>This list only includes the platforms that the Percona Operators are specifically tested on as part of the release process. Other Kubernetes flavors and versions depend on the backward compatibility offered by Kubernetes itself.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.4.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.0.html","title":"Percona Operator for PostgreSQL 1.5.0","text":"<ul> <li> <p>Date</p> <p>December 11, 2023</p> </li> <li> <p>Installation</p> <p>Percona Operator for PostgreSQL</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.0.html#release-highlights","title":"Release highlights","text":"<p>This release contains a number of fixes and improvements made within the maintenance mode that the Operator 1.x is in.</p> <p>The Operator 1.x goes end-of-life in July, 2024, so we strongly recommend to use Percona Operator for PostgreSQL 2.x instead. The Operator version 2 has newer PostgreSQL versions, new features and improvements, which will not find their way to the Operator 1.x version.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.0.html#improvements","title":"Improvements","text":"<ul> <li>{{ k8spgjira(340) }}: To continuously improve the Operator we capture anonymous telemetry and usage data. In this release we add more data points to it</li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.0.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li> <p>{{ k8spgjira(420) }}: Fix a bug due to which pausing and unpausing the cluster after modification of Custom Resource could result in wrong scale of replica and backrest repo Pods</p> </li> <li> <p>{{ k8spgjira(314) }}: Version Service at check.percona.com was was incorrectly parsing the version string which lead to issues with automated upgrades</p> </li> <li> <p>{{ k8spgjira(404) }}: Fix a bug due to which upgrading the Operator version 1.3 to 1.4 could cause the cluster to have no replicas</p> </li> <li> <p>{{ k8spgjira(464) }}: Our Affinity configuration was not taking components into account. This led to unschedulable Pods that were stuck in Pending state. It is fixed in this release through adding component labels</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.0.html#supported-platforms","title":"Supported platforms","text":"<p>The Operator was developed and tested with PostgreSQL versions 12.16, 13.12, and 14.9. Other options may also work but have not been tested. The Operator 1.5.0 provides connection pooling based on pgBouncer 1.20.0 and high-availability implementation based on Patroni 2.1.4.</p> <p>The following platforms were tested and are officially supported in this release:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.24 - 1.28</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.24 - 1.28</p> </li> <li> <p>OpenShift 4.11 - 4.14</p> </li> <li> <p>Minikube 1.32</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.0.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.1.html","title":"Percona Operator for PostgreSQL 1.5.1","text":"<ul> <li> <p>Date</p> <p>January 29, 2024</p> </li> <li> <p>Installation</p> <p>Percona Operator for PostgreSQL</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.1.html#release-highlights","title":"Release Highlights","text":"<p>This release provides fixes for the following vulnerabilities in  PostgreSQL, pgBackRest, and pgBouncer images used by the Operator:</p> <ul> <li>OpenSSH could cause remote code execution by ssh-agent if a user establishes an SSH connection to a compromised or malicious SSH server and has agent forwarding enabled (CVE-2023-38408). This vulnerability affects pgBackRest and PostgreSQL images.</li> <li>The c-ares library could cause a Denial of Service with 0-byte UDP payload (CVE-2023-32067). This vulnerability affects pgBouncer image.</li> </ul> <p>Users of the Operator version 1.x are recommended to upgrade to 1.5.1 to resolve these issues.</p>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.1.html#bugs-fixed","title":"Bugs Fixed","text":"<ul> <li>{{ k8spgjira(494) }}: Fix vulnerabilities in PostgreSQL, pgBackRest, and pgBouncer images</li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.1.html#supported-platforms","title":"Supported platforms","text":"<p>The Operator was developed and tested with PostgreSQL versions 12.16, 13.12, and 14.9. Other options may also work but have not been tested. The Operator 1.5.1 provides connection pooling based on pgBouncer 1.20.0 and high-availability implementation based on Patroni 2.1.4.</p> <p>The following platforms were tested and are officially supported in this release:</p> <ul> <li> <p>Google Kubernetes Engine (GKE) 1.24 - 1.28</p> </li> <li> <p>Amazon Elastic Container Service for Kubernetes (EKS) 1.24 - 1.28</p> </li> <li> <p>OpenShift 4.11 - 4.14</p> </li> <li> <p>Minikube 1.32</p> </li> </ul>"},{"location":"ReleaseNotes/Kubernetes-Operator-for-PostgreSQL-RN1.5.1.html#get-expert-help","title":"Get expert help","text":"<p>If you need assistance, visit the community forum for comprehensive and free database knowledge, or contact our Percona Database Experts for professional support and services. Join K8S Squad to benefit from early access to features and \u201cask me anything\u201d sessions with the Experts. </p> <p> Community Forum  Get a Percona Expert</p>"}]}